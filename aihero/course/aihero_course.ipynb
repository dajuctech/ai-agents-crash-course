{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc365db5",
   "metadata": {},
   "source": [
    "# GitHub Repository Data Ingestion\n",
    "\n",
    "Building a conversational AI agent for GitHub repositories. This implementation downloads and processes markdown documentation from any repository.\n",
    "\n",
    "**Target repositories:**\n",
    "- DataTalksClub/faq - Course FAQ documentation\n",
    "- evidentlyai/docs - Evidently AI library documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fc810",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c994f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b32c36",
   "metadata": {},
   "source": [
    "## Understanding Frontmatter\n",
    "\n",
    "Frontmatter is YAML metadata in markdown files, commonly used in Jekyll, Hugo, and Next.js.\n",
    "\n",
    "Structure:\n",
    "```yaml\n",
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "tags: [\"ai\", \"machine-learning\"]\n",
    "---\n",
    "\n",
    "# Content here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82fdb089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with AI\n",
      "['ai', 'machine-learning']\n",
      "# Getting Started with AI\n",
      "\n",
      "Main content here.\n"
     ]
    }
   ],
   "source": [
    "# Parsing frontmatter\n",
    "example = \"\"\"---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "tags: [\"ai\", \"machine-learning\"]\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "Main content here.\n",
    "\"\"\"\n",
    "\n",
    "post = frontmatter.loads(example)\n",
    "\n",
    "print(post.metadata['title'])\n",
    "print(post.metadata['tags'])\n",
    "print(post.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c69a4",
   "metadata": {},
   "source": [
    "## Download Repository as Zip\n",
    "\n",
    "GitHub provides zip downloads via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83d1361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc380e",
   "metadata": {},
   "source": [
    "## Process Zip in Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "058b1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "caf711c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# DataTalks.Club FAQ\\n\\nA static site generator for DataTalks.Club course FAQs with automated AI-powered FAQ maintenance.\\n\\n## Features\\n\\n- **Static Site Generation**: Converts markdown FAQs to a beautiful, searchable HTML site\\n- **Automated FAQ Management**: AI-powered bot that processes new FAQ proposals\\n- **Intelligent Triage**: Automatically determines if proposals should create new entries, update existing ones, or are duplicates\\n- **GitHub Integration**: Seamless workflow via GitHub Issues and Pull Requests\\n\\n## Project Structure\\n\\n```\\nfaq/\\n├── _questions/              # FAQ content organized by course\\n│   ├── machine-learning-zoomcamp/\\n│   │   ├── _metadata.yaml   # Course configuration\\n│   │   ├── general/         # General course questions\\n│   │   ├── module-1/        # Module-specific questions\\n│   │   └── ...\\n│   ├── data-engineering-zoomcamp/\\n│   └── ...\\n├── _layouts/                # Jinja2 HTML templates\\n│   ├── base.html\\n│   ├── course.html\\n│   └── index.html\\n├── assets/                  # CSS and static assets\\n├── faq_automation/          # FAQ automation module\\n│   ├── core.py             # Core FAQ processing functions\\n│   ├── rag_agent.py        # AI-powered decision agent\\n│   ├── actions.py          # GitHub Actions integration\\n│   └── cli.py              # Command-line interface\\n├── tests/                   # Test suite\\n├── generate_website.py      # Main site generator\\n└── Makefile                # Build commands\\n```\\n\\n## Contributing FAQ Entries\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions.\\n\\n\\n## Development\\n\\n### Setup\\n\\n```bash\\n# Install dependencies\\nuv sync --dev\\n```\\n\\nFor testing the FAQ automation locally, you\\'ll need to set your OpenAI API key:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key-here\\'\\n```\\n\\nOr add it to your shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`).\\n\\n### Running Locally\\n\\nTo test the FAQ automation locally, create a `test_issue.txt` file:\\n\\n```bash\\ncat > test_issue.txt << \\'EOF\\'\\n### Course\\nmachine-learning-zoomcamp\\n\\n### Question\\nHow do I check my Python version?\\n\\n### Answer\\nRun `python --version` in your terminal.\\nEOF\\n```\\n\\nThen process the FAQ proposal:\\n\\n```bash\\nuv run python -m faq_automation.cli \\\\\\n  --issue-body \"$(cat test_issue.txt)\" \\\\\\n  --issue-number 42\\n```\\n\\n### Testing\\n\\n```bash\\n# Generate static website\\nmake website\\n\\n# Run all tests\\nmake test\\n\\n# Run unit tests only\\nmake test-unit\\n\\n# Run integration tests only\\nmake test-int\\n```\\n\\nSee [testing documentation](tests/README.md) for detailed information about the test suite, including how to run specific test files or methods, test coverage details, and guidelines for adding new tests.\\n\\n## Architecture\\n\\n### Site Generation Pipeline\\n\\n1. **Collection** (`collect_questions()`):\\n   - Reads all markdown files from `_questions/`\\n   - Parses YAML frontmatter\\n   - Loads course metadata for section ordering\\n\\n2. **Processing** (`process_markdown()`):\\n   - Converts markdown to HTML\\n   - Applies syntax highlighting (Pygments)\\n   - Auto-links plain text URLs\\n   - Handles image placeholders\\n\\n3. **Sorting** (`sort_sections_and_questions()`):\\n   - Orders sections per `_metadata.yaml`\\n   - Sorts questions by `sort_order` field\\n\\n4. **Rendering** (`generate_site()`):\\n   - Applies Jinja2 templates\\n   - Generates course pages and index\\n   - Copies assets to `_site/`', 'filename': 'faq-main/readme.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08a025",
   "metadata": {},
   "source": [
    "## Reusable Function\n",
    "\n",
    "Handles both `.md` and `.mdx` files for broader compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48e2a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse markdown files from GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with content and metadata\n",
    "    \"\"\"\n",
    "    url = f'https://codeload.github.com/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404927b",
   "metadata": {},
   "source": [
    "## Download Target Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bda3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1232\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40319f2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "FAQ data is ready for indexing. Evidently docs require chunking due to large document size (20k+ characters).\n",
    "\n",
    "Chunking breaks large documents into smaller pieces for:\n",
    "- Better search relevance\n",
    "- Improved AI model performance\n",
    "- Token limit compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c8658",
   "metadata": {},
   "source": [
    "## Document Chunking\n",
    "\n",
    "Large documents require splitting into smaller pieces. Evidently docs average 20k+ characters, which creates problems with token limits, costs, and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2214060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'LLM regression testing', 'description': 'How to run regression testing for LLM outputs.', 'content': 'In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You\\'ll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.', 'filename': 'docs-main/examples/LLM_regression_testing.mdx'}\n",
      "\n",
      "Content length: 21712 characters\n"
     ]
    }
   ],
   "source": [
    "# Inspect sample document\n",
    "print(evidently_docs[45])\n",
    "print(f\"\\nContent length: {len(evidently_docs[45]['content'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ebc13",
   "metadata": {},
   "source": [
    "## 1. Sliding Window Chunking\n",
    "\n",
    "Simple approach: cut into equal pieces with overlap.\n",
    "\n",
    "Without overlap (problematic):\n",
    "- Chunk 1: 0..2000\n",
    "- Chunk 2: 2000..4000\n",
    "- Issues: context loss, incomplete sentences\n",
    "\n",
    "With overlap (recommended):\n",
    "- Chunk 1: 0..2000\n",
    "- Chunk 2: 1000..3000\n",
    "- Chunk 3: 2000..4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "367e6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4aa08eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 21 chunks\n"
     ]
    }
   ],
   "source": [
    "# Test on document 45\n",
    "text = evidently_docs[45]['content']\n",
    "chunks = sliding_window(text, 2000, 1000)\n",
    "print(f\"Generated {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ba9a4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 576\n"
     ]
    }
   ],
   "source": [
    "# Process all documents\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225e86d",
   "metadata": {},
   "source": [
    "Result: 575 chunks from 95 documents. Size 2000 is typically sufficient for RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c93de",
   "metadata": {},
   "source": [
    "## 2. Paragraph Splitting\n",
    "\n",
    "Uses regex pattern `\\n\\s*\\n` to split on double newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "035e7033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 153\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "print(f\"Paragraphs: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b686e",
   "metadata": {},
   "source": [
    "Works for literature but less effective for technical docs with short paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9653a7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9eae5ab",
   "metadata": {},
   "source": [
    "## 3. Section Splitting\n",
    "\n",
    "Splits by markdown headers using document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59038a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        header = parts[i] + parts[i+1]\n",
    "        header = header.strip()\n",
    "\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06908d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 8\n"
     ]
    }
   ],
   "source": [
    "# Test section splitting\n",
    "text = evidently_docs[45]['content']\n",
    "sections = split_markdown_by_level(text, level=2)\n",
    "print(f\"Sections: {len(sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c31f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 266\n"
     ]
    }
   ],
   "source": [
    "# Process all documents\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "print(f\"Sections: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d593fa",
   "metadata": {},
   "source": [
    "## 4. LLM-Based Chunking\n",
    "\n",
    "Use when:\n",
    "- Complex, non-standard document structure\n",
    "- Semantic coherence required\n",
    "- Quality prioritized over cost\n",
    "\n",
    "Requires OpenAI API key:\n",
    "```bash\n",
    "export OPENAI_API_KEY='your-api-key'\n",
    "uv add openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa054d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7c0cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "031a1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80478817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d2aebe8ce43a0a09f75fbc0d926c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM chunks: 741\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "print(f\"LLM chunks: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ea85f",
   "metadata": {},
   "source": [
    "## Chunking Strategy\n",
    "\n",
    "Start simple with sliding window. Evaluate performance, then consider section-based or LLM chunking if needed. Next: indexing data into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4eb405",
   "metadata": {},
   "source": [
    "## Search Implementation\n",
    "\n",
    "This section covers indexing and searching the processed data. Three approaches are implemented:\n",
    "- Text search for exact matches\n",
    "- Vector search for semantic similarity\n",
    "- Hybrid search combining both methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91d167",
   "metadata": {},
   "source": [
    "## 1. Text Search\n",
    "\n",
    "Text search finds documents containing query words. More matching words indicate higher relevance.\n",
    "\n",
    "Install minsearch:\n",
    "```bash\n",
    "uv add minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4052b258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x11f0ae850>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "# Index Evidently chunks\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e777c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx', 'section': '## Create a RAG Test Dataset\\n\\nYou can generate a ground truth RAG dataset from your data source through the following steps.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![Select Method](https://via.placeholder.com/150)\\n\\n### 2. Upload Your Knowledge Base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs. Choose how many inputs to generate.\\n\\n![Upload Example](https://via.placeholder.com/150)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![Upload Example](https://via.placeholder.com/150)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the Test Cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![Test Case Review](https://via.placeholder.com/150)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>'}\n"
     ]
    }
   ],
   "source": [
    "# Test search\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "145e80f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x11eff5940>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index FAQ data\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bce8c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n"
     ]
    }
   ],
   "source": [
    "query = 'Can I still join the course after the start date?'\n",
    "results = faq_index.search(query)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4213d5c",
   "metadata": {},
   "source": [
    "## 2. Vector Search\n",
    "\n",
    "Vector search uses embeddings to find semantically similar documents, even when words differ.\n",
    "\n",
    "Install sentence-transformers:\n",
    "```bash\n",
    "uv add sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a31b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c9e47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.5190933346748352\n"
     ]
    }
   ],
   "source": [
    "# Create document embedding\n",
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n",
    "\n",
    "# Create query embedding\n",
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n",
    "\n",
    "# Compute similarity\n",
    "similarity = v_query.dot(v_doc)\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f9c512df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c00249c01df46b6818932986bfb7e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Create embeddings for all FAQ documents\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2662f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x32810ccd0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "55c72b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n"
     ]
    }
   ],
   "source": [
    "# Test vector search\n",
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9107f227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65be4363b7bd4f9ebcc888eece58d2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x32810cb90>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vector index for Evidently docs\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f48e3",
   "metadata": {},
   "source": [
    "## 3. Hybrid Search\n",
    "\n",
    "Combines text and vector search for optimal results.\n",
    "\n",
    "**Text search advantages:**\n",
    "- Fast and efficient\n",
    "- Works well for exact matches\n",
    "- No model inference needed\n",
    "\n",
    "**Vector search advantages:**\n",
    "- Captures semantic meaning\n",
    "- Handles paraphrased questions\n",
    "- Works with synonyms\n",
    "\n",
    "**Hybrid search:** Best of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44db52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 10\n"
     ]
    }
   ],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "# Get results from both methods\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "# Combine results\n",
    "final_results = text_results + vector_results\n",
    "print(f\"Total results: {len(final_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce8b69",
   "metadata": {},
   "source": [
    "## 4. Search Functions\n",
    "\n",
    "Organizing search code into reusable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a208ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['id'] not in seen_ids:\n",
    "            seen_ids.add(result['id'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7258dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text search:\n",
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n",
      "\n",
      "Vector search:\n",
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n",
      "\n",
      "Hybrid search:\n",
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n"
     ]
    }
   ],
   "source": [
    "# Test search functions\n",
    "query = 'Can I still enroll in the course?'\n",
    "\n",
    "print(\"Text search:\")\n",
    "print(text_search(query)[0])\n",
    "\n",
    "print(\"\\nVector search:\")\n",
    "print(vector_search(query)[0])\n",
    "\n",
    "print(\"\\nHybrid search:\")\n",
    "print(hybrid_search(query)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86dd2cc",
   "metadata": {},
   "source": [
    "## Search Strategy\n",
    "\n",
    "Start with text search - it's faster, easier to debug, and works well for many cases. Add vector or hybrid search only when text search proves insufficient. Next: building the conversational agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77f6cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63531335",
   "metadata": {},
   "source": [
    "## Agents and Tools\n",
    "\n",
    "An agent is an LLM that can invoke tools - external functions for retrieving information, performing calculations, or taking actions. Tool access makes systems \"agentic\".\n",
    "\n",
    "This section implements an agent using the search function created previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a850d7d",
   "metadata": {},
   "source": [
    "## 1. LLM Without Tools\n",
    "\n",
    "Testing LLM response without tool access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11be91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It depends on the specific course you’re interested in. Many courses have set enrollment periods or deadlines, while others may allow rolling admissions or open enrollment. If it's an online course, you might be able to join anytime. I recommend checking the course provider's website or contacting their support team for the most accurate information regarding enrollment options.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae7321",
   "metadata": {},
   "source": [
    "Response is generic without access to course-specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d87135",
   "metadata": {},
   "source": [
    "## 2. Function Calling with OpenAI\n",
    "\n",
    "Implementing tool access through OpenAI's function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e6877e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function description for OpenAI\n",
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"text_search\",\n",
    "        \"description\": \"Search the FAQ database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "268bc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageFunctionToolCall(id='call_j91VXQdG5aumf9KJmOFVfRzx', function=Function(arguments='{\"query\":\"join the course\"}', name='text_search'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9bfc7481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even after the start date. It is recommended that you register before the course starts, which for the next cohort is on January 13th, 2025. However, if you miss the registration, you are still eligible to submit homework assignments. Do keep in mind that there will be deadlines for submitting homework and final projects, so it's a good idea not to procrastinate.\n",
      "\n",
      "If you're interested, you can register using [this link](https://airtable.com/shr6oVXeQvSI5HuWD) and join the [course Telegram channel](https://t.me/dezoomcamp) for announcements.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Execute the tool call\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "arguments = json.loads(tool_call.function.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "# Format response\n",
    "chat_messages.append(response.choices[0].message)\n",
    "chat_messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": json.dumps(result)\n",
    "})\n",
    "\n",
    "# Get final response\n",
    "response = openai_client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17773695",
   "metadata": {},
   "source": [
    "## 3. System Prompt Instructions\n",
    "\n",
    "System prompts control agent behavior. More detailed instructions typically yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c734a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57a315",
   "metadata": {},
   "source": [
    "## 4. Pydantic AI\n",
    "\n",
    "Pydantic AI simplifies agent implementation by handling function calling complexity.\n",
    "\n",
    "Install:\n",
    "```bash\n",
    "uv add pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98ed67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5e0be38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[text_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "de6373a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even after it has started. Although registering is preferred, you are eligible to submit homework without registration. However, keep in mind that there will be deadlines for submitting homework and final projects, so it’s advisable to manage your time effectively and not leave everything until the last minute.\n",
      "\n",
      "For more information, you can check [here](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html) to register and get updates.\n"
     ]
    }
   ],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "#result = agent.run_sync(question)\n",
    "result = await agent.run(question)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "38bf0ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content=\"\\nYou are a helpful assistant for a course. \\n\\nUse the search tool to find relevant information from the course materials before answering questions.\\n\\nIf you can find specific information through search, use it to provide accurate answers.\\nIf the search doesn't return relevant results, let the user know and provide general guidance.\\n\", timestamp=datetime.datetime(2026, 1, 20, 13, 26, 3, 870509, tzinfo=datetime.timezone.utc)), UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2026, 1, 20, 13, 26, 3, 870520, tzinfo=datetime.timezone.utc))], run_id='bfc8e3a6-4d37-4499-89fc-6642f93f4554'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"join course late enrollment\"}', tool_call_id='call_UGkTsGAUjM2eE1Jh5XH7enT5')], usage=RequestUsage(input_tokens=162, output_tokens=17, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2026, 1, 20, 13, 26, 4, tzinfo=TzInfo(0)), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-D0628ZbOoobMa5HECiMDwT9suTeJl', finish_reason='tool_call', run_id='bfc8e3a6-4d37-4499-89fc-6642f93f4554'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '4dbd2eea47', 'question': 'Homework: Are late submissions of homework allowed?', 'sort_order': 17, 'content': 'No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}], tool_call_id='call_UGkTsGAUjM2eE1Jh5XH7enT5', timestamp=datetime.datetime(2026, 1, 20, 13, 26, 4, 824404, tzinfo=datetime.timezone.utc))], run_id='bfc8e3a6-4d37-4499-89fc-6642f93f4554'),\n",
       " ModelResponse(parts=[TextPart(content='Yes, you can still join the course even after it has started. Although registering is preferred, you are eligible to submit homework without registration. However, keep in mind that there will be deadlines for submitting homework and final projects, so it’s advisable to manage your time effectively and not leave everything until the last minute.\\n\\nFor more information, you can check [here](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html) to register and get updates.')], usage=RequestUsage(input_tokens=849, output_tokens=104, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2026, 1, 20, 13, 26, 5, tzinfo=TzInfo(0)), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-D0629UO13TE2UgBOb7LAxAJ0H4roV', finish_reason='stop', run_id='bfc8e3a6-4d37-4499-89fc-6642f93f4554')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect agent reasoning\n",
    "result.all_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10279f6f",
   "metadata": {},
   "source": [
    "Pydantic AI handles conversation history, tool execution, and response parsing automatically. Next: evaluating agent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b83ab2",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Agent performance requires systematic measurement. This section covers:\n",
    "- Logging system for tracking interactions\n",
    "- Automated evaluation using LLM as judge\n",
    "- Test data generation\n",
    "- Performance metrics calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feea381",
   "metadata": {},
   "source": [
    "## 1. Logging System\n",
    "\n",
    "Manual testing doesn't scale. A logging system records agent interactions for systematic evaluation.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "14d7a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8aa665e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_obj = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    ts_str = ts_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d164ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test interactive logging\n",
    "question = input(\"Enter question: \")\n",
    "result = await agent.run(question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.all_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce0d85",
   "metadata": {},
   "source": [
    "## 2. Adding References\n",
    "\n",
    "Adjusting system prompt to include source citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create v2 agent\n",
    "agent = Agent(\n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[text_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24a912",
   "metadata": {},
   "source": [
    "## 3. LLM as Judge\n",
    "\n",
    "Using one LLM to evaluate another's outputs through structured criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e27899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa781ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea935819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate a log\n",
    "log_record = load_log_file('./logs/faq_agent_v2_20250120_120000_abc123.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n",
    "\n",
    "result = await eval_agent.run(user_prompt)\n",
    "\n",
    "checklist = result.data\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf88953",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt)\n",
    "    return result.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b599f248",
   "metadata": {},
   "source": [
    "## 4. Data Generation\n",
    "\n",
    "Generating test questions using AI to expand evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=question_generation_prompt,\n",
    "    output_type=QuestionsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.data.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4503265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.all_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb319f",
   "metadata": {},
   "source": [
    "## 5. Batch Evaluation\n",
    "\n",
    "Evaluating multiple logs and calculating aggregate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff489aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "34a02634",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_evals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate average pass rates\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf_evals\u001b[49m.mean(numeric_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_evals' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate average pass rates\n",
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34692b6",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "Average pass rates indicate agent performance across criteria. Key metric: `answer_relevant` shows whether the agent addresses user questions correctly. These metrics guide improvements to prompts, search methods, and chunking approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcfad5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
