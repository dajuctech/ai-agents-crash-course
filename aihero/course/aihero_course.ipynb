{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b460f0f",
   "metadata": {},
   "source": [
    "## **Day 1: Ingest and Index Your Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcac65",
   "metadata": {},
   "source": [
    "**Setting up the imports**\n",
    "\n",
    "Bring in the libraries we need. ``requests`` grabs the repo from GitHub, ``frontmatter`` pulls metadata out of markdown files, and ``zipfile/io`` let us unpack the zip without saving it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307ffc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Projects/datatalks/AI Agents Crash Course/aihero/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c994f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b32c36",
   "metadata": {},
   "source": [
    "**Testing how frontmatter works**\n",
    "\n",
    "Quick test to see how frontmatter works. The stuff between the ``---`` markers is YAML metadata, and everything below is the actual content.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fdb089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Getting Started with AI\n",
      "Tags: ['ai', 'machine-learning']\n",
      "Content: # Getting Started with AI\n",
      "\n",
      "Main content here.\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "tags: [\"ai\", \"machine-learning\"]\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "Main content here.\n",
    "\"\"\"\n",
    "\n",
    "post = frontmatter.loads(example)\n",
    "\n",
    "print(\"Title:\", post.metadata['title'])\n",
    "print(\"Tags:\", post.metadata['tags'])\n",
    "print(\"Content:\", post.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c69a4",
   "metadata": {},
   "source": [
    "**Downloading the repo as a zip**\n",
    "\n",
    "Download the DataTalksClub FAQ repo as a zip. GitHub gives us a handy URL pattern for this so we don't have to clone anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d1361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n"
     ]
    }
   ],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n",
    "print(f\"Status: {resp.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc380e",
   "metadata": {},
   "source": [
    "**Processing the zip in memory**\n",
    "\n",
    "Open the zip straight from memory, loop through every ``.md`` file, parse its frontmatter, and collect everything into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058b1812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1271 markdown files\n"
     ]
    }
   ],
   "source": [
    "repository_data = []\n",
    "\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with zf.open(file_info) as f_in:\n",
    "            content = f_in.read()\n",
    "            post = frontmatter.loads(content)\n",
    "            data = post.to_dict()\n",
    "            data['filename'] = filename\n",
    "            repository_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "zf.close()\n",
    "\n",
    "print(f\"Found {len(repository_data)} markdown files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f479d",
   "metadata": {},
   "source": [
    "**Checking what we got**\n",
    "\n",
    "Look at one record to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf711c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# FAQ Bot Feedback - PR Review Corrections\\n\\n## 1. Wrong Section Placement\\n\\nKestra-related FAQs were incorrectly placed in `general` or `module-1` instead of `module-2` (workflow orchestration):\\n\\n| PR | Issue | Correction |\\n|----|-------|------------|\\n| #141 | Kestra IANA timezones | general → module-2, sort_order 20 |\\n| #137 | Kestra stdout variables | general → module-2, sort_order 21 |\\n| #135 | Kestra outputFiles visibility | general → module-2, sort_order 22 |\\n| #118 | Kestra Docker socket | module-1 → module-2, sort_order 23 |\\n\\n**Rule**: Kestra questions belong in `module-2` (workflow orchestration), not `general` or `module-1`.\\n\\n---\\n\\n## 2. Not Relevant for Course (closed)\\n\\n| PR | Topic | Reason |\\n|----|-------|--------|\\n| #123 | Installing vim on Ubuntu | Basic Linux admin, outside course scope |\\n| #116 | SQL LEFT JOIN returns NULL | Basic SQL concept, not course-specific |\\n\\n**Rule**: Fundamental tool/SQL concepts that aren\\'t course-specific should be rejected.\\n\\n---\\n\\n## 3. Duplicates (closed)\\n\\n| PR | Issue | Duplicate of |\\n|----|-------|--------------|\\n| #114 | Docker localhost/pgnetwork | PR #104 (DE zoomcamp) |\\n| #99 | Spark Global Temporary Views | PR #100 (DE zoomcamp) |\\n\\n**Rule**: Check for duplicates across both DE and ML zoomcamp before creating new entries. Same content should not exist in both courses.\\n\\n---\\n\\n## 4. Content Merges\\n\\n| PR | Issue | Action |\\n|----|-------|--------|\\n| #110 | Codespaces pgAdmin blank screen | Merged into existing DE zoomcamp FAQ instead of creating separate ML zoomcamp entry |\\n\\n**Rule**: Enhance existing entries rather than duplicating across courses.\\n\\n---\\n\\n## 5. Sort Order Issues (fixed before merge)\\n\\n| PR | Issue | Fix |\\n|----|-------|-----|\\n| #159 | Set sort_order to 1, highest in module-3 is 045 | Changed to 046 |\\n| #157 | Set sort_order to 46, conflicted with #159 | Changed to 047 |\\n\\n**Rule**: Always check existing files in target section and use next sequential number.\\n\\n---\\n\\n## 6. Code/File Issues (fixed before merge)\\n\\n| PR | Issue | Fix |\\n|----|-------|-----|\\n| #102 | Kafka Python code had no indentation | Fixed proper Python indentation |\\n| #94 | File corruption (extra closing braces at end) | Removed garbage characters |\\n\\n---\\n\\n## 7. Bot Failure - Issue #128\\n\\nThe FAQ bot crashed with:\\n```\\nValueError: invalid literal for int() with base 10: \\'05727a95dd\\'\\n```\\n\\n**Root cause**: Malformed filename `05727a95dd_homework-and-leaderboard-wha.md` - missing underscore between sort_order and doc_id. Should be `057_27a95dd_...`.\\n\\nThe `find_largest_sort_order()` function in `faq_automation/core.py` doesn\\'t handle malformed filenames robustly.\\n\\n---\\n\\n## Implementation Details for Bot Fixes\\n\\n### Fix 1: Robust Sort Order Parsing (`faq_automation/core.py`)\\n\\n**Current code (line 134-143)**:\\n```python\\ndef find_largest_sort_order(section_dir: Path) -> int:\\n    last = sorted(section_dir.iterdir())[-1]\\n    sort_order, _ = last.name.split(\\'_\\', maxsplit=1)\\n    return int(sort_order) + 1\\n```\\n\\n**Fixed code**:\\n```python\\nimport re\\n\\ndef find_largest_sort_order(section_dir: Path) -> int:\\n    \"\"\"\\n    Find the next available sort order number in a section\\n\\n    Handles malformed filenames by extracting the numeric prefix only.\\n    \"\"\"\\n    files = list(section_dir.glob(\\'*.md\\'))\\n    if not files:\\n        return 1\\n\\n    max_order = 0\\n    for f in files:\\n        # Extract numeric prefix from filename (e.g., \"123_\" from \"123_abc.md\")\\n        match = re.match(r\\'^(\\\\d+)\\', f.name)\\n        if match:\\n            order = int(match.group(1))\\n            max_order = max(max_order, order)\\n\\n    return max_order + 1\\n```\\n\\n---\\n\\n### Fix 2: Section Placement Rules (`faq_automation/rag_agent.py`)\\n\\n**Add to SYSTEM_PROMPT** (after line 50):\\n```python\\nSection Placement Rules:\\n- Kestra-related questions (workflows, tasks, outputs, Docker socket) → module-2 (workflow orchestration)\\n- Docker + Kestra → module-2 (Kestra is the primary topic)\\n- Docker-only questions (pgAdmin, Postgres, etc.) → module-1\\n- BigQuery, GCP, data warehousing → module-3\\n- Kafka, streaming → module-6\\n- Spark → module-5\\n- Generic questions that truly don\\'t fit any module → general\\n```\\n\\n---\\n\\n### Fix 3: Relevance Filtering (`faq_automation/rag_agent.py`)\\n\\n**Add to SYSTEM_PROMPT** (after Section Placement Rules):\\n```python\\nRejection Rules:\\n- Basic Linux administration (vim installation, package management) → REJECT\\n- Fundamental SQL concepts (LEFT JOIN behavior, basic syntax) → REJECT\\n- Generic DevOps best practices (scaling FastAPI, Docker reproducibility) without course-specific context → REJECT\\n- Content that is general programming documentation rather than course-specific troubleshooting → REJECT\\n```\\n\\n---\\n\\n### Fix 4: Cross-Course Duplicate Check\\n\\n**New function in `faq_automation/core.py`**:\\n```python\\ndef check_cross_course_duplicates(question: str, answer: str, course_dirs: List[Path]) -> Optional[dict]:\\n    \"\"\"\\n    Check if similar content exists in other courses\\n\\n    Returns the first matching document across all courses, or None\\n    \"\"\"\\n    from minsearch import Index\\n\\n    all_docs = []\\n    for course_dir in course_dirs:\\n        all_docs.extend(read_questions(course_dir))\\n\\n    if not all_docs:\\n        return None\\n\\n    # Build combined index\\n    index = Index(\\n        text_fields=[\\'section\\', \\'question\\', \\'answer\\'],\\n        keyword_fields=[\\'course\\', \\'section_id\\'],\\n    )\\n    index.fit(all_docs)\\n\\n    # Search for duplicates\\n    proposal = f\"## {question}\\\\n\\\\n{answer}\"\\n    results = index.search(proposal, num_results=3)\\n\\n    # High similarity threshold for duplicate detection\\n    if results and results[0].get(\\'score\\', 0) > 0.85:\\n        return results[0]\\n\\n    return None\\n```\\n\\n---\\n\\n### Fix 5: Sort Order When order == -1\\n\\n**In `faq_automation/actions.py`**, modify `create_new_faq_file()`:\\n```python\\ndef create_new_faq_file(course_dir: Path, doc_index: dict, faq_decision):\\n    # ... existing code ...\\n\\n    # Handle sort_order\\n    if faq_decision.order == -1:\\n        sort_order = find_largest_sort_order(section_dir)\\n    else:\\n        sort_order = faq_decision.order\\n        # Validate the requested order doesn\\'t conflict\\n        existing = section_dir.glob(f\"{sort_order:03d}_*.md\")\\n        if list(existing):\\n            faq_decision.warnings.append(f\"Sort order {sort_order} conflicts with existing file\")\\n            sort_order = find_largest_sort_order(section_dir)\\n```\\n\\n---\\n\\n## Recommendations for FAQ Bot\\n\\n1. **Kestra questions** → `module-2` (workflow orchestration)\\n2. **Basic Linux/SQL tutorials** → reject as not course-specific\\n3. **Check for duplicates** across both DE and ML zoomcamp before creating new entries\\n4. **ML zoomcamp** should only get content that\\'s actually ML-specific; Docker/infrastructure questions belong in DE zoomcamp\\n5. **Sort order** - always find highest number in target section first (`ls _questions/<course>/<section>/`), use next sequential number\\n6. **Validate code blocks** - ensure proper indentation for Python and other languages\\n7. **Check for file corruption** - validate file content doesn\\'t have garbage at the end', 'filename': 'faq-main/.claude/feedback.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08a025",
   "metadata": {},
   "source": [
    "**Building a reusable function**\n",
    "\n",
    "Wrap everything into a function so we can reuse it on any repo. This version also handles ``.mdx`` files (React-flavored markdown that Evidently uses) and has basic error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e2a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Download failed with status {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404927b",
   "metadata": {},
   "source": [
    "**Pulling data from both repos**\n",
    "\n",
    "Put it to work. Pull docs from two repos and see how many files we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bda3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1271\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b285323",
   "metadata": {},
   "source": [
    "That's all of Day 1. The FAQ data is small enough to use as-is. The Evidently docs are much larger and need chunking, which is Day 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ff899",
   "metadata": {},
   "source": [
    "## **Day 2: Chunking and Intelligent Processing for Data**\n",
    "\n",
    "**Why we need chunking**\n",
    "\n",
    "The Evidently docs are way too large to feed directly into an LLM. Large documents cause problems with token limits, cost, performance, and relevance. So we break them into smaller pieces — that's what chunking is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1441477",
   "metadata": {},
   "source": [
    "**Looking at a sample document**\n",
    "\n",
    "Before we chunk anything, let's see what we're dealing with. Document 45 from the Evidently repo is a good example of a large doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a106cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'LLM regression testing', 'description': 'How to run regression testing for LLM outputs.', 'content': 'In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You\\'ll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.', 'filename': 'docs-main/examples/LLM_regression_testing.mdx'}\n",
      "\n",
      "Content length: 21712 characters\n"
     ]
    }
   ],
   "source": [
    "print(evidently_docs[45])\n",
    "print(f\"\\nContent length: {len(evidently_docs[45]['content'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835014c3",
   "metadata": {},
   "source": [
    "**1. Simple Chunking (Sliding Window)**\n",
    "\n",
    "Cuts text into fixed-size pieces with overlap between them. The overlap (controlled by ``step`` being smaller than ``size``) means we don't lose context at the boundaries. With size 2000 and step 1000, neighboring chunks share 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e4f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df916e",
   "metadata": {},
   "source": [
    "**Testing it on one document 45**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e093d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 21 chunks\n"
     ]
    }
   ],
   "source": [
    "text = evidently_docs[45]['content']\n",
    "chunks = sliding_window(text, 2000, 1000)\n",
    "print(f\"Generated {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20d5e4",
   "metadata": {},
   "source": [
    "**Chunking or Processing all the Evidently docs**\n",
    "\n",
    "We loop through each document, pull out the content with ``pop('content'),`` chunk it, then attach the remaining metadata (title, description, etc.) to every chunk so we know where it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d5c7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 576\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks: {len(evidently_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e968a",
   "metadata": {},
   "source": [
    "**2. Splitting by Paragraphs and Sections**\n",
    "\n",
    "**Paragraph splitting**\n",
    "\n",
    "Uses a regex pattern that matches double newlines — the standard way paragraphs are separated in text files. Works great for prose, less so for technical docs where paragraphs tend to be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68ab1211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 153\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "print(f\"Paragraphs: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9a141",
   "metadata": {},
   "source": [
    "**Section splitting function**\n",
    "\n",
    "Instead of splitting by character count, this takes advantage of how markdown is actually structured. It finds ``##`` headers and splits on those, so each chunk maps to a real topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b5f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        header = parts[i] + parts[i+1]\n",
    "        header = header.strip()\n",
    "\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5abba",
   "metadata": {},
   "source": [
    "**Testing section splitting on document 45**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d83f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 8\n"
     ]
    }
   ],
   "source": [
    "text = evidently_docs[45]['content']\n",
    "sections = split_markdown_by_level(text, level=2)\n",
    "print(f\"Sections: {len(sections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c47a64",
   "metadata": {},
   "source": [
    "**Applying section splitting to all docs**\n",
    "\n",
    "Same pattern as before — pull out the content, split it, then reattach the metadata to each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edeb7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 266\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "print(f\"Sections: {len(evidently_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d489a",
   "metadata": {},
   "source": [
    "**3. Intelligent Chunking with LLM**\n",
    "\n",
    "This is the expensive option — hand the document to GPT and let it decide how to split things up. Only worth doing when simpler methods aren't good enough and you have the budget for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcec7f6",
   "metadata": {},
   "source": [
    "**Loading your API key from .env**\n",
    "\n",
    "Run this first so OpenAI can authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e16f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "print(\"Key loaded:\", \"OPENAI_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad309dfa",
   "metadata": {},
   "source": [
    "**Setting up the OpenAI client and LLM helper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "313529fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4682a",
   "metadata": {},
   "source": [
    "**The prompt template**\n",
    "\n",
    "Tells the LLM to break the document into logical, self-contained sections and separate them with ``---`` so we can parse the output easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04e0ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bbe92",
   "metadata": {},
   "source": [
    "**The intelligent chunking function**\n",
    "\n",
    "Sends the document to the LLM, gets back the split result, then chops it on the ``---`` separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fb0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fd80c",
   "metadata": {},
   "source": [
    "**Running it on all Evidently docs**\n",
    "\n",
    "This takes a while and costs money. ``tqdm`` gives you a progress bar so you can track how far along it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c60c3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebd12c7d9d545b8b665550975f325af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM chunks: 702\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "print(f\"LLM chunks: {len(evidently_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08793a23",
   "metadata": {},
   "source": [
    "**How to Choose a Chunking Approach**\n",
    "\n",
    "Start with the simplest one and only increase complexity if needed:\n",
    "\n",
    "1. **Sliding window** — try this first, it handles most cases\n",
    "2. **Section splitting** — use when your docs have clear markdown structure\n",
    "3. **LLM chunking** — only when simpler methods produce poor results and you have the budget\n",
    "\n",
    "That's Day 2. The takeaway is to start with the sliding window — it handles most cases fine. Only move to section splitting or LLM chunking if your search results aren't good enough after you evaluate them. Day 3 covers putting all this data into a search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcd7ce",
   "metadata": {},
   "source": [
    "## **Day 3: Add Search**\n",
    "\n",
    "\n",
    "Our data is downloaded and chunked. Now we put it inside a search engine so we can quickly find relevant information when users ask questions. Three approaches: text search, vector search, and hybrid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abd09d",
   "metadata": {},
   "source": [
    "#### **1. Text Search**\n",
    "\n",
    "**Install minsearch**\n",
    "\n",
    "Run this in your terminal first:\n",
    "\n",
    "``aihero/venv/bin/python -m pip install minsearch``\n",
    "\n",
    "**Reminder — preparing the Evidently chunks**\n",
    "\n",
    "If you're starting fresh or restarted your kernel, you need the chunks from Day 1 and Day 2 loaded first. This is what the data looks like going into search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54bbdb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b61db",
   "metadata": {},
   "source": [
    "**Indexing Evidently chunks**\n",
    "\n",
    "We create an index that searches through four text fields: the chunk content, title, description, and filename. Then we feed all our chunks into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407d3e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x136491160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135048b",
   "metadata": {},
   "source": [
    "**Searching the Evidently index**\n",
    "\n",
    "Text search finds documents containing words from the query. More matching words means higher relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6de3b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}\n"
     ]
    }
   ],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf00e9",
   "metadata": {},
   "source": [
    "**Indexing the FAQ data**\n",
    "\n",
    "For the DataTalksClub FAQ, we don't need chunking — the entries are small enough already. We filter for just the data engineering course questions and search through the question and content fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c24741f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x111733d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494549df",
   "metadata": {},
   "source": [
    "**Searching the FAQ index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7027616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n"
     ]
    }
   ],
   "source": [
    "query = 'Can I still join the course after the start date?'\n",
    "results = faq_index.search(query)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d0b6c",
   "metadata": {},
   "source": [
    "#### **2. Vector Search**\n",
    "\n",
    "Text search has a blind spot — if someone asks \"can I still enroll?\" instead of \"can I still join?\", text search might miss it because the words are different even though the meaning is the same. Vector search fixes this by turning text into numerical representations (embeddings) that capture meaning, not just exact words.\n",
    "\n",
    "**Install sentence-transformers**\n",
    "\n",
    "Run in your terminal:\n",
    "\n",
    "``aihero/venv/bin/python -m pip install sentence-transformers``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e8067",
   "metadata": {},
   "source": [
    "**Loading the embedding model**\n",
    "\n",
    "The ``multi-qa-distilbert-cos-v1`` model is trained specifically for question-answering tasks. It creates embeddings where similar meanings end up close together in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bd321e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1982d5",
   "metadata": {},
   "source": [
    "**How embeddings and similarity work**\n",
    "\n",
    "We combine the question and answer text from one FAQ entry, turn it into a vector, do the same for a query, and compute how similar they are using the dot product. Values closer to 1 mean high similarity, closer to 0 means low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a230085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.5190933346748352\n"
     ]
    }
   ],
   "source": [
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n",
    "\n",
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n",
    "\n",
    "similarity = v_query.dot(v_doc)\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53172e3",
   "metadata": {},
   "source": [
    "**Creating embeddings for all FAQ documents**\n",
    "\n",
    "This loops through every FAQ entry, combines the question and answer text, and turns each one into a vector. Takes a bit of time so ``tqdm`` gives us a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81b9af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55fad5e333346838f67a5505aa16a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ea1a9",
   "metadata": {},
   "source": [
    "**Building the vector search index**\n",
    "\n",
    "``VectorSearch`` from minsearch takes our embeddings and the original documents, then lets us find the most similar ones to any query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a37cbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1423bd160>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258224d",
   "metadata": {},
   "source": [
    "**Testing vector search**\n",
    "\n",
    "We encode the query into a vector first, then search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4152a3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n"
     ]
    }
   ],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2fa91",
   "metadata": {},
   "source": [
    "**Creating a vector index for Evidently docs**\n",
    "\n",
    "Same process but for the Evidently chunks. We only use the ``chunk`` field for embeddings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e52d94ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4054de2eec0f484e98829df3aa709c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1576f2fd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7f0e5",
   "metadata": {},
   "source": [
    "#### **3. Hybrid Search**\n",
    "\n",
    "Text search is fast and catches exact keywords. Vector search understands meaning and handles synonyms. Combining both gives us the best of both worlds — that's hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64fbccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 10\n"
     ]
    }
   ],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results\n",
    "print(f\"Total results: {len(final_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd37bbd",
   "metadata": {},
   "source": [
    "#### **4. Putting This Together**\n",
    "\n",
    "**Organizing search into reusable functions**\n",
    "\n",
    "Before we can hand this off to an agent, we need clean functions it can call. The hybrid search also deduplicates results so we don't get the same document twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1d69506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf39032",
   "metadata": {},
   "source": [
    "**Testing all three search functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87ad77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text search:\n",
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n",
      "\n",
      "Vector search:\n",
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n",
      "\n",
      "Hybrid search:\n",
      "{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}\n"
     ]
    }
   ],
   "source": [
    "query = 'Can I still enroll in the course?'\n",
    "\n",
    "print(\"Text search:\")\n",
    "print(text_search(query)[0])\n",
    "\n",
    "print(\"\\nVector search:\")\n",
    "print(vector_search(query)[0])\n",
    "\n",
    "print(\"\\nHybrid search:\")\n",
    "print(hybrid_search(query)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f872a06",
   "metadata": {},
   "source": [
    "#### **5. Selecting the Best Approach**\n",
    "\n",
    "Start simple with text search — it's faster, easier to debug, and works well for many use cases. Only add vector or hybrid search when text search isn't cutting it. We'll cover evaluation methods later in the course to help make that decision with data instead of guessing.\n",
    "\n",
    "That's all of Day 3. You now have a working search system with three approaches. Day 4 covers building the conversational agent that actually uses these search functions to answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e02591",
   "metadata": {},
   "source": [
    "## **Day 4: Agents and Tools**\n",
    "\n",
    "We spent three days preparing data. Now we finally build the agent that uses it. An agent is just an LLM that can call external functions (tools) to look things up. Without tools, it's just a chatbot guessing. With tools, it can actually look up answers.\n",
    "\n",
    "#### **1. LLM Without Tools**\n",
    "\n",
    "**What happens when you ask a question with no search access**\n",
    "\n",
    "The LLM has no idea about your specific course — it just gives a generic answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cd3a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if you can join the course at this point, please check the following:\n",
      "\n",
      "1. **Enrollment Deadlines**: Look for any deadlines for enrollment. Many courses have specific start and end dates.\n",
      "\n",
      "2. **Prerequisites**: Some courses may have prerequisites that you need to meet before joining.\n",
      "\n",
      "3. **Availability**: Check if the course is still accepting new students. Some courses may have a cap on enrollment.\n",
      "\n",
      "4. **Instructor's Policies**: Review the course policies provided by the instructor or institution regarding late enrollment.\n",
      "\n",
      "If you're unsure, it's best to reach out directly to the course instructor or administrative office for guidance.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33fd0b2",
   "metadata": {},
   "source": [
    "#### **2. Function Calling with OpenAI**\n",
    "\n",
    "**Defining the search function**\n",
    "\n",
    "This is the text search from Day 3. Make sure ``faq_index`` is already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c21c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad8e60",
   "metadata": {},
   "source": [
    "**Describing the function for OpenAI**\n",
    "\n",
    "We can't just pass a Python function to OpenAI. We need to describe it in a structured format so the LLM knows what it does, what parameters it takes, and when to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51e0b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f39f4c",
   "metadata": {},
   "source": [
    "**Sending the question with the tool available**\n",
    "\n",
    "Now when we ask a question, the LLM can decide to call our search function instead of guessing. We pass the tool description alongside the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ee5cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6a1ab",
   "metadata": {},
   "source": [
    "**Inspecting the tool call**\n",
    "\n",
    "The LLM didn't answer directly — it decided it needs to search first. Let's see what it wants to search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cf8c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"query\":\"join course\"}', call_id='call_x6S47Hpd2ekhfHinRkVI9yGF', name='text_search', type='function_call', id='fc_0fb2e36e70b772c500698dd007e9d881948f815b3770721194', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db0c5e",
   "metadata": {},
   "source": [
    "**Executing the tool call and sending results back**\n",
    "\n",
    "The LLM told us what function to call and with what arguments. We run the function, package the results, and send everything back so the LLM can formulate a proper answer.\n",
    "\n",
    "LLMs are stateless — each API call is independent. So we need to send the full conversation history: the original system prompt, the user question, the tool call decision, and the tool results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b0bc6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even after it has started! Here are some details:\n",
      "\n",
      "- **Eligibility:** You can submit homework even if you don't register, but do note that there will be deadlines for turning in homeworks and the final projects.\n",
      "- **Next Start Date:** The next cohort starts on January 12th, 2026. You can register before this date using the [registration link](https://airtable.com/shr6oVXeQvSI5HuWD).\n",
      "\n",
      "Feel free to join the course announcements on [Telegram](https://t.me/dezoomcamp) and make sure to register in DataTalks.Club's Slack! If you have more questions or need further assistance, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n",
    "\n",
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a5b0d",
   "metadata": {},
   "source": [
    "#### **3. System Prompt: Instructions**\n",
    "\n",
    "**Why the system prompt matters**\n",
    "\n",
    "The system prompt controls how the agent behaves. More complete instructions generally produce better results. Here's a more detailed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00414134",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683c473",
   "metadata": {},
   "source": [
    "**Encouraging multiple searches**\n",
    "\n",
    "If you want the agent to try harder when the first search doesn't give enough information, you can adjust the prompt like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61816b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f1596",
   "metadata": {},
   "source": [
    "#### **4. Pydantic AI**\n",
    "\n",
    "**Why use a framework**\n",
    "\n",
    "Manually handling function calls, parsing responses, and managing conversation history is tedious and error-prone. Pydantic AI handles all of that for us.\n",
    "\n",
    "**Install pydantic-ai**\n",
    "\n",
    "Run in your terminal:\n",
    "\n",
    "\n",
    " ``aihero/venv/bin/python -m pip install pydantic-ai``\n",
    " \n",
    "\n",
    "**Redefining text_search with type hints**\n",
    "\n",
    "Pydantic AI needs type hints and docstrings on the function — it uses those to automatically generate the tool description (instead of us writing the JSON by hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fbe1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d3256",
   "metadata": {},
   "source": [
    "**Creating the agent**\n",
    "\n",
    "We pass the function directly — no JSON description needed. Pydantic AI figures it out from the type hints and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64a92014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0550826",
   "metadata": {},
   "source": [
    "**Running the agent**\n",
    "\n",
    "In a Jupyter notebook we use ``await`` because Pydantic AI is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09c00a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even after it has started. While registration is encouraged, you are eligible to submit homework assignments even if you don't officially register. However, keep in mind that there are deadlines for submitting homework and final projects, so it's best to stay on schedule and not leave things until the last minute.\n"
     ]
    }
   ],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c37abb",
   "metadata": {},
   "source": [
    "**If you're NOT in Jupyter**\n",
    "\n",
    "If you're running this as a regular Python script instead of a notebook, use ``asyncio.run():``\n",
    "\n",
    "\n",
    "``import asyncio``\n",
    "\n",
    "``result = asyncio.run(agent.run(user_prompt=question))``\n",
    "\n",
    "``print(result.output)``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07c42e",
   "metadata": {},
   "source": [
    "**Looking inside the agent's reasoning**\n",
    "\n",
    "This shows the full breakdown of what happened — the user prompt, the tool call decision, the search results, and the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9427dd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2026, 2, 12, 13, 5, 23, 860682, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\", run_id='667491e8-90e5-4729-84a9-0abfaf1d9fe8'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"Can I join the course now?\"}', tool_call_id='call_2vcAebfxvb89AgJmxUJmXh1D')], usage=RequestUsage(input_tokens=146, output_tokens=20, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2026, 2, 12, 13, 5, 24, tzinfo=TzInfo(0)), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-D8Qfk6szKxABPvx3nCbyTLUmhmIVO', finish_reason='tool_call', run_id='667491e8-90e5-4729-84a9-0abfaf1d9fe8'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 12th, 2026. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and [join](https://datatalks.club/docs/general/slack/) the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}], tool_call_id='call_2vcAebfxvb89AgJmxUJmXh1D', timestamp=datetime.datetime(2026, 2, 12, 13, 5, 24, 862814, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\", run_id='667491e8-90e5-4729-84a9-0abfaf1d9fe8'),\n",
       " ModelResponse(parts=[TextPart(content=\"Yes, you can still join the course even after it has started. While registration is encouraged, you are eligible to submit homework assignments even if you don't officially register. However, keep in mind that there are deadlines for submitting homework and final projects, so it's best to stay on schedule and not leave things until the last minute.\")], usage=RequestUsage(input_tokens=855, output_tokens=66, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2026, 2, 12, 13, 5, 25, tzinfo=TzInfo(0)), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-D8QflrcvVDT3InMEeLuLNEFPEarBm', finish_reason='stop', run_id='667491e8-90e5-4729-84a9-0abfaf1d9fe8')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d702e",
   "metadata": {},
   "source": [
    "That's all of Day 4. You now have a working agent that can look up real answers from your data instead of guessing. Day 5 covers how to evaluate whether the agent is actually doing a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7fead",
   "metadata": {},
   "source": [
    "## **Day 5: Evaluation**\n",
    "\n",
    "We built the agent yesterday. But is it actually good? Today we build a proper evaluation system: logging, LLM as a judge, test data generation, and metrics.\n",
    "\n",
    "#### **Logging**\n",
    "\n",
    "**Setting up the agent (recap from Day 4)**\n",
    "\n",
    "If you restarted your kernel, you need this running first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a672683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe5825",
   "metadata": {},
   "source": [
    "**Testing the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "910f17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install Kafka-related libraries in Python, you can follow these steps:\n",
      "\n",
      "1. **Install `confluent-kafka`:**\n",
      "   - Using pip:\n",
      "     ```bash\n",
      "     pip install confluent-kafka\n",
      "     ```\n",
      "   - Using conda:\n",
      "     ```bash\n",
      "     conda install conda-forge::python-confluent-kafka\n",
      "     ```\n",
      "\n",
      "2. **Install `fastavro` (if required):**\n",
      "   ```bash\n",
      "   pip install fastavro\n",
      "   ```\n",
      "\n",
      "3. **(Optional) Install `kafka-python`:** \n",
      "   If needed, you can install a specific version of `kafka-python`:\n",
      "   ```bash\n",
      "   pip install kafka-python==1.4.6\n",
      "   ```\n",
      "\n",
      "4. **(Alternative) If you encounter issues:**\n",
      "   You might consider installing an alternative package:\n",
      "   ```bash\n",
      "   pip install kafka-python-ng\n",
      "   ```\n",
      "\n",
      "These commands will set up the necessary dependencies to work with Kafka in Python. Make sure you have Python and pip installed on your system before executing these commands.\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814d106",
   "metadata": {},
   "source": [
    "**Building the log entry extractor**\n",
    "\n",
    "This pulls out everything we want to record from the agent and the run result: the config, tools, and full message history. ``ModelMessagesTypeAdapter`` converts internal Pydantic AI message objects into regular Python dictionaries so we can save them as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be01ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3233ba",
   "metadata": {},
   "source": [
    "**Writing logs to files**\n",
    "\n",
    "Creates a ``logs`` folder, generates unique filenames using timestamp and random hex, and saves each interaction as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06564463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_obj = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    ts_str = ts_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2f689",
   "metadata": {},
   "source": [
    "**Interactive vibe checking with logging**\n",
    "\n",
    "Type a question, get a response, and the interaction gets saved to a log file automatically. Try questions like: \"how do I use docker on windows?\", \"can I join late and get a certificate?\", \"what do I need to do for the certificate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30f578de-7e39-4e9e-ae0f-918ce3eb27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    \n",
    "    if isinstance(ts, str):\n",
    "        ts_obj = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    else:\n",
    "        ts_obj = ts\n",
    "    \n",
    "    ts_str = ts_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c546e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " history\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find specific information related to \"history\" in the course materials. \n",
      "\n",
      "If you're looking for the historical context of a certain topic within the course, please provide a bit more detail so I can assist you better. Otherwise, I can offer general guidance on history-related topics relevant to the course subject.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_v2_20260212_130616_9ac490.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd38590",
   "metadata": {},
   "source": [
    "#### **Adding References**\n",
    "\n",
    "**Updating the system prompt to include citations**\n",
    "\n",
    "When vibe checking, we noticed the agent doesn't include references to source documents. We fix that by telling it to cite filenames and link to the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04b50394",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebd820",
   "metadata": {},
   "source": [
    "#### **LLM as a Judge**\n",
    "\n",
    "**The evaluation prompt**\n",
    "\n",
    "Instead of manually checking every response, we use another LLM to evaluate our agent's output. The prompt defines a checklist of things to check: did it follow instructions, is the answer relevant, does it include citations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18604ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fc421",
   "metadata": {},
   "source": [
    "**Defining the structured output**\n",
    "\n",
    "We use Pydantic models so the evaluation LLM returns a well-defined structure we can process programmatically. Putting ``justification`` before ``check_pass`` makes the LLM reason about the answer before giving its final judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "972cb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aebb1b7",
   "metadata": {},
   "source": [
    "**Creating the evaluation agent**\n",
    "\n",
    "We use a different model ``(gpt-5-nano)`` to evaluate our agent's output ``(gpt-4o-mini)``. Using a different model reduces self-bias and gives a second opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fa2899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc1677",
   "metadata": {},
   "source": [
    "**The evaluation input template**\n",
    "\n",
    "We use XML tags so the LLM can clearly see the boundaries between the instructions, the question, the answer, and the full conversation log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7568e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9c87e",
   "metadata": {},
   "source": [
    "**Loading log files**\n",
    "\n",
    "A helper to load a saved JSON log and add the filename for tracking.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8df674e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ae243-c3b5-4f92-a584-097b5c36105d",
   "metadata": {},
   "source": [
    "**Check what log files exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ceee8477-88f1-428d-aa45-79d7e1097d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faq_agent_v2_20260212_130616_9ac490.json\n",
      "faq_agent_20260212_130551_00991a.json\n",
      "faq_agent_20260210_205544_311005.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in os.listdir('./logs'):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0605f4d",
   "metadata": {},
   "source": [
    "**Running the evaluation on a single log**\n",
    "\n",
    "Load a log file, extract the key pieces, format them into the evaluation prompt, and run the eval agent. Replace the filename with one of your actual log files from the ``logs/`` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b853bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool search initiated to locate 'history' references in course materials. Awaiting results to craft a precise answer with citations or provide general guidance if not found.\n",
      "check_name='instructions_follow' justification=\"We will use a search tool as instructed to look for 'history' in course materials before answering.\" check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed content; we will only search and provide guidance.' check_pass=True\n",
      "check_name='answer_relevant' justification='We will search and then answer with relevant course material references if found; otherwise provide general guidance.' check_pass=True\n",
      "check_name='answer_clear' justification='We will provide a concise, clear response with references or guidance.' check_pass=True\n",
      "check_name='answer_citations' justification='We will cite the filename(s) from the course materials using the required format when used.' check_pass=True\n",
      "check_name='completeness' justification='We will cover both search results and offer general guidance or narrowing questions.' check_pass=True\n",
      "check_name='tool_call_search' justification='We invoked a search tool as required.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "log_record = load_log_file('./logs/faq_agent_v2_20260212_130616_9ac490.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n",
    "\n",
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffbb4a",
   "metadata": {},
   "source": [
    "**Simplifying log messages to save tokens**\n",
    "\n",
    "The full conversation log has a lot of noise (timestamps, IDs, full search results). We strip that out to reduce token cost and speed up evaluation. The actual search results get replaced with a placeholder since the eval agent doesn't need them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02d524f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2c33f",
   "metadata": {},
   "source": [
    "**Putting evaluation together into one function**\n",
    "\n",
    "Combines loading, simplifying, formatting, and running the eval into a single reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d28acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1a81c",
   "metadata": {},
   "source": [
    "**Testing the combined function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "502fddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record = load_log_file('./logs/faq_agent_v2_20260212_130616_9ac490.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9424f-bd38-4747-ae77-08ce6c1aafce",
   "metadata": {},
   "source": [
    "**Viewing the evaluation results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26251798-228a-4e57-a7ef-65b8cd95a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to search course materials for 'history' to provide cited references.\n"
     ]
    }
   ],
   "source": [
    "print(eval1.summary)\n",
    "\n",
    "for check in eval1.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53458b35",
   "metadata": {},
   "source": [
    "#### **Data Generation**\n",
    "\n",
    "**Creating a question generator**\n",
    "\n",
    "Instead of manually typing questions, we get AI to generate realistic test questions based on our FAQ content. We sample records from the database, feed them to the generator, and it creates questions a student might actually ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3486998",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983da428",
   "metadata": {},
   "source": [
    "**Generating questions from sampled records**\n",
    "\n",
    "We pick 10 random FAQ entries, extract their content, and ask the generator to produce one question per entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6811aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef46cc2",
   "metadata": {},
   "source": [
    "**Running the agent on generated questions and logging results**\n",
    "\n",
    "Each generated question gets sent to our agent, the response gets printed, and everything gets logged with the source marked as ``'ai-generated'`` so we can filter for it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "847b2f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c9b98d2d0143a7bf419bddb11cdc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should I do if I encounter a java.lang.IllegalAccessError when running `spark-shell` after installing Java, Hadoop, and Spark?\n",
      "If you encounter a `java.lang.IllegalAccessError` when running `spark-shell` after installing Java, Hadoop, and Spark, it’s likely due to an unsupported Java version. Specifically, this error can occur because:\n",
      "\n",
      "- Spark 3.x requires Java 8, 11, or 16. If you are using Java 17 or 19, it will not be compatible.\n",
      "  \n",
      "To resolve the issue:\n",
      "\n",
      "1. **Install Java 11**: You can download and install Java 11 from the official website.\n",
      "2. After installation, ensure that your `JAVA_HOME` environment variable is set to point to the Java 11 installation and that your `PATH` variable includes the bin directory of the Java installation.\n",
      "\n",
      "Here’s a reference for detailed guidance: [Spark-shell: unable to load native-hadoop library for platform - Windows](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-5/004_a3790900cc_spark-shell-unable-to-load-native-hadoop-library-f.md)\n",
      "\n",
      "Are there any recommendations for resources on Data Engineering that I could check out?\n",
      "Here are some recommended resources on Data Engineering that you can check out:\n",
      "\n",
      "1. **Awesome Data Engineering Resources**: This document includes a comprehensive list of resources related to data engineering. It covers various topics and tools in the field. You can access it [here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md).\n",
      "\n",
      "2. **YouTube Playlists**: The \"Data Engineering Zoomcamp\" has several playlists that provide video content relevant to data engineering concepts, tutorials, and updates. Some of the main playlists include:\n",
      "   - [Data Engineering Zoomcamp](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n",
      "   - [Data Engineering Zoomcamp 2022](https://www.youtube.com/playlist?list=PL3MmuxUbc_hKVX8VnwWCPaWlIHf1qmg8s)\n",
      "   - [Data Engineering Zoomcamp 2023](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n",
      "\n",
      "These resources should provide a solid foundation and ongoing education in Data Engineering. For further details, please refer to the source materials: \n",
      "- [Awesome Data Engineering Resources](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md)\n",
      "- [YouTube playlists](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/010_8ac65b2225_course-which-playlist-on-youtube-should-i-refer-to.md)\n",
      "\n",
      "What precautions should I take when using BigQuery to avoid unexpected bills?\n",
      "When using BigQuery, there are several precautions you can take to avoid unexpected bills:\n",
      "\n",
      "1. **Monitor Your Billing**: Regularly check your billing information, especially if you have recently created datasets or virtual machines (VMs). This will help you catch any unexpected charges early. It’s advisable to monitor billing daily.\n",
      "\n",
      "2. **Free Trial Caution**: Be cautious when using BigQuery after your Google Cloud free trial has expired. It's easy to incur charges without realizing it. For example, one user experienced an $80 bill because they created a dataset after their free trial ended.\n",
      "\n",
      "3. **Destroy Unused Datasets**: If you are utilizing BigQuery under free credits, ensure that you delete any datasets after your work is completed to avoid ongoing charges.\n",
      "\n",
      "By implementing these strategies, you can manage your usage and expenditures more effectively in BigQuery. \n",
      "\n",
      "For more detailed guidance, you can check the specific document referencing these precautions: [GCP: Caution in using BigQuery](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-3/009_0131ac93ac_gcp-caution-in-using-bigquery-bigqueryno.md).\n",
      "\n",
      "Where can I find deadlines for the Data Engineering course in 2025?\n",
      "Deadlines for the Data Engineering course in 2025 will be announced on [the course website](https://courses.datatalks.club/de-zoomcamp-2025/) and can also be accessed through [Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ). \n",
      "\n",
      "Additionally, make sure to watch for updates from @Au-Tomator as they may provide announcements about any extensions or changes to deadlines. \n",
      "\n",
      "You can also refer to the deadlines for the previous year (2024) through this [2024 Deadlines Spreadsheet](https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml). \n",
      "\n",
      "For further details, see the source [Homework: What are homework and project deadlines?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/016_d3f485cd10_homework-what-are-homework-and-project-deadlines.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210280f",
   "metadata": {},
   "source": [
    "**Collecting AI-generated logs for evaluation**\n",
    "\n",
    "We filter the log files to only include AI-generated interactions from the v2 agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3ce5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37418f2f",
   "metadata": {},
   "source": [
    "**Running evaluation on all logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc5d2c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee34e493a14e4af18d61be2261653ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7c62d",
   "metadata": {},
   "source": [
    "**Converting results to a DataFrame**\n",
    "\n",
    "We extract the question, answer, and each check's pass/fail result into rows so Pandas can work with them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "573ac138",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec68fb",
   "metadata": {},
   "source": [
    "**Viewing results and calculating metrics**\n",
    "\n",
    "Install pandas if you haven't:\n",
    "\n",
    "\n",
    "``aihero/venv/bin/python -m pip install pandas``\n",
    "\n",
    "Then create the DataFrame and calculate average pass rates per check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa5f5776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faq_agent_v2_20260212_131434_c5f21b.json</td>\n",
       "      <td>Where can I find deadlines for the Data Engine...</td>\n",
       "      <td>Deadlines for the Data Engineering course in 2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faq_agent_v2_20260212_131420_421684.json</td>\n",
       "      <td>Are there any recommendations for resources on...</td>\n",
       "      <td>Here are some recommended resources on Data En...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faq_agent_v2_20260212_131427_955b66.json</td>\n",
       "      <td>What precautions should I take when using BigQ...</td>\n",
       "      <td>When using BigQuery, there are several precaut...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faq_agent_v2_20260212_131414_1b4d72.json</td>\n",
       "      <td>What should I do if I encounter a java.lang.Il...</td>\n",
       "      <td>If you encounter a `java.lang.IllegalAccessErr...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0  faq_agent_v2_20260212_131434_c5f21b.json   \n",
       "1  faq_agent_v2_20260212_131420_421684.json   \n",
       "2  faq_agent_v2_20260212_131427_955b66.json   \n",
       "3  faq_agent_v2_20260212_131414_1b4d72.json   \n",
       "\n",
       "                                            question  \\\n",
       "0  Where can I find deadlines for the Data Engine...   \n",
       "1  Are there any recommendations for resources on...   \n",
       "2  What precautions should I take when using BigQ...   \n",
       "3  What should I do if I encounter a java.lang.Il...   \n",
       "\n",
       "                                              answer  instructions_follow  \\\n",
       "0  Deadlines for the Data Engineering course in 2...                 True   \n",
       "1  Here are some recommended resources on Data En...                False   \n",
       "2  When using BigQuery, there are several precaut...                 True   \n",
       "3  If you encounter a `java.lang.IllegalAccessErr...                 True   \n",
       "\n",
       "  instructions_avoid answer_relevant answer_clear answer_citations  \\\n",
       "0               True            True         True             True   \n",
       "1               True            True         True             True   \n",
       "2                NaN             NaN          NaN              NaN   \n",
       "3               True            True         True             True   \n",
       "\n",
       "  completeness tool_call_search  \n",
       "0         True             True  \n",
       "1         True             True  \n",
       "2          NaN              NaN  \n",
       "3         True             True  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9e488",
   "metadata": {},
   "source": [
    "**Overall pass rates**\n",
    "\n",
    "This gives you the big picture — what percentage of responses pass each check. The most important one is ``answer_relevant:`` does the agent actually answer the user's question?\n",
    "\n",
    "\n",
    "``df_evals.mean(numeric_only=True)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aedf6d",
   "metadata": {},
   "source": [
    "#### **Evaluating Search Quality (Bonus)**\n",
    "\n",
    "**Hit rate and MRR calculation**\n",
    "\n",
    "This is a standalone function you can use to evaluate your search function separately from the agent. It checks whether at least one relevant document shows up in results (hit rate) and where it ranks (MRR). You'd need to prepare test queries with expected document filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ff7c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "    \n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query, num_results=5)\n",
    "        \n",
    "        # Calculate hit rate\n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "            \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb98e7",
   "metadata": {},
   "source": [
    "That's all of Day 5. The big takeaway: start with manual vibe checks, log everything, then automate evaluation with LLM as a judge. Use the metrics to compare different prompts, search methods, and chunking strategies. Day 6 covers building a UI and deploying the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01233772",
   "metadata": {},
   "source": [
    "## **Day 6: Publish Your Agent**\n",
    "\n",
    "The agent lives in a Jupyter notebook right now. Time to clean it up, give it a proper UI, and put it on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed54ab-aafd-4a69-9e92-2ed2f1093543",
   "metadata": {},
   "source": [
    "### **Cleaning Up**\n",
    "\n",
    "**Project structure**\n",
    "\n",
    "First, create a folder called app inside your ``course`` directory (you may already have one). Inside it, initialize a new uv project:\n",
    "\n",
    "\n",
    "``cd app``\n",
    "\n",
    "``uv init``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129d2b1-7080-4f77-98e8-5a38383e85d4",
   "metadata": {},
   "source": [
    "**Dependencies in pyproject.toml**\n",
    "\n",
    "Update the ``dependencies`` section in your ``app/pyproject.toml:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5679fdf7-84f9-4adc-8c76-36e604983e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = [\n",
    "    \"minsearch>=0.0.5\",\n",
    "    \"openai>=1.108.2\",\n",
    "    \"pydantic-ai==1.0.9\",\n",
    "    \"python-frontmatter>=1.1.0\",\n",
    "    \"requests>=2.32.5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfd242-7a95-4b23-8a43-c279d87010e0",
   "metadata": {},
   "source": [
    "``Then install them:``\n",
    "\n",
    "``uv sync``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441638c-36f8-42b3-88fd-aaa716b36027",
   "metadata": {},
   "source": [
    "``ingest.py``\n",
    "\n",
    "Handles downloading repos, chunking, and indexing. This combines everything from Days 1-3 into one file. A couple of improvements: the zip archive prefix gets stripped from filenames, and the sliding window uses ``content`` instead of ``chunk`` so the field name is consistent whether you chunk or not."
   ]
  },
  {
   "cell_type": "raw",
   "id": "07a194f3-6f80-400b-8fd2-a72f8606df88",
   "metadata": {},
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    url = f'https://codeload.github.com/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    repository_data = []\n",
    "\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename.lower()\n",
    "\n",
    "        if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "            continue\n",
    "\n",
    "        with zf.open(file_info) as f_in:\n",
    "            content = f_in.read()\n",
    "            post = frontmatter.loads(content)\n",
    "            data = post.to_dict()\n",
    "\n",
    "            _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "            data['filename'] = filename_repo\n",
    "            repository_data.append(data)\n",
    "\n",
    "    zf.close()\n",
    "\n",
    "    return repository_data\n",
    "\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(docs, size=2000, step=1000):\n",
    "    chunks = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop('content')\n",
    "        doc_chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in doc_chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        chunks.extend(doc_chunks)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def index_data(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        filter=None,\n",
    "        chunk=False,\n",
    "        chunking_params=None,\n",
    "    ):\n",
    "    docs = read_repo_data(repo_owner, repo_name)\n",
    "\n",
    "    if filter is not None:\n",
    "        docs = [doc for doc in docs if filter(doc)]\n",
    "\n",
    "    if chunk:\n",
    "        if chunking_params is None:\n",
    "            chunking_params = {'size': 2000, 'step': 1000}\n",
    "        docs = chunk_documents(docs, **chunking_params)\n",
    "\n",
    "    index = Index(\n",
    "        text_fields=[\"content\", \"filename\"],\n",
    "    )\n",
    "\n",
    "    index.fit(docs)\n",
    "    return index``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca650f-26e8-4add-b41a-cbe56815ca61",
   "metadata": {},
   "source": [
    "``search_tools.py``\n",
    "\n",
    "The search function wrapped in a class instead of relying on a global variable. The index gets passed in at creation time."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dc11799-0731-47b3-83db-c620bbda9e31",
   "metadata": {},
   "source": [
    "from typing import List, Any\n",
    "\n",
    "class SearchTool:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, query: str) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Perform a text-based search on the FAQ index.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query string.\n",
    "\n",
    "        Returns:\n",
    "            List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "        \"\"\"\n",
    "        return self.index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e3fff-fc34-4fff-93e6-cfa07c4afa44",
   "metadata": {},
   "source": [
    "``search_agent.py``\n",
    "\n",
    "Creates the Pydantic AI agent. The system prompt is now a template so it works with any GitHub repo, not just hardcoded to one."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f3c7558-aa17-44c6-8a1e-62c66563bf3f",
   "metadata": {},
   "source": [
    "import search_tools\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that answers questions about documentation.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.\n",
    "Replace it with the full path to the GitHub repository:\n",
    "\"https://github.com/{repo_owner}/{repo_name}/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "def init_agent(index, repo_owner, repo_name):\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(repo_owner=repo_owner, repo_name=repo_name)\n",
    "\n",
    "    search_tool = search_tools.SearchTool(index=index)\n",
    "\n",
    "    agent = Agent(\n",
    "        name=\"gh_agent\",\n",
    "        instructions=system_prompt,\n",
    "        tools=[search_tool.search],\n",
    "        model='gpt-4o-mini'\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd070d-6751-44b7-9ada-017104dce8a5",
   "metadata": {},
   "source": [
    "``logs.py``\n",
    "\n",
    "Same logging from Day 5, but the log directory is now configurable through an environment variable. Useful when deploying."
   ]
  },
  {
   "cell_type": "raw",
   "id": "44fb5489-a863-47b2-b4f9-43810eb0f83e",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "LOG_DIR = Path(os.getenv('LOGS_DIRECTORY', 'logs'))\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e4efc-ebd0-4ec3-b7ae-6795c4c024a1",
   "metadata": {},
   "source": [
    "``main.py`` **— Command-line interface**\n",
    "\n",
    "Brings everything together. Downloads the data, indexes it, creates the agent, and runs an interactive loop in the terminal. Type ``stop`` to exit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d537c1b-7aeb-40a4-8779-5e2a045865ea",
   "metadata": {},
   "source": [
    "import ingest\n",
    "import search_agent \n",
    "import logs\n",
    "\n",
    "import asyncio\n",
    "\n",
    "\n",
    "REPO_OWNER = \"DataTalksClub\"\n",
    "REPO_NAME = \"faq\"\n",
    "\n",
    "\n",
    "def initialize_index():\n",
    "    print(f\"Starting AI FAQ Assistant for {REPO_OWNER}/{REPO_NAME}\")\n",
    "    print(\"Initializing data ingestion...\")\n",
    "\n",
    "    def filter(doc):\n",
    "        return 'data-engineering' in doc['filename']\n",
    "\n",
    "    index = ingest.index_data(REPO_OWNER, REPO_NAME, filter=filter)\n",
    "    print(\"Data indexing completed successfully!\")\n",
    "    return index\n",
    "\n",
    "\n",
    "def initialize_agent(index):\n",
    "    print(\"Initializing search agent...\")\n",
    "    agent = search_agent.init_agent(index, REPO_OWNER, REPO_NAME)\n",
    "    print(\"Agent initialized successfully!\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "def main():\n",
    "    index = initialize_index()\n",
    "    agent = initialize_agent(index)\n",
    "    print(\"\\nReady to answer your questions!\")\n",
    "    print(\"Type 'stop' to exit the program.\\n\")\n",
    "\n",
    "    while True:\n",
    "        question = input(\"Your question: \")\n",
    "        if question.strip().lower() == 'stop':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(\"Processing your question...\")\n",
    "        response = asyncio.run(agent.run(user_prompt=question))\n",
    "        logs.log_interaction_to_file(agent, response.new_messages())\n",
    "\n",
    "        print(\"\\nResponse:\\n\", response.output)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1cf83-e179-40c7-b90f-f9b5968f1872",
   "metadata": {},
   "source": [
    "**Running the CLI**\n",
    "\n",
    "From the ``app`` folder:\n",
    "\n",
    "``uv run python main.py``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a9760-b9e2-483d-87c0-38167424ee65",
   "metadata": {},
   "source": [
    "##### **Streamlit UI**\n",
    "\n",
    "**Install Streamlit**\n",
    "\n",
    "``uv add streamlit``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04daecb-7d8d-4cae-9d49-5c9d5d519821",
   "metadata": {},
   "source": [
    "**app.py — Basic version**\n",
    "\n",
    "This is the simple version where the full response appears at once."
   ]
  },
  {
   "cell_type": "raw",
   "id": "459066f5-4b9c-4fab-ad04-561c6ec7cabf",
   "metadata": {},
   "source": [
    "import streamlit as st\n",
    "import asyncio\n",
    "\n",
    "import ingest\n",
    "import search_agent\n",
    "import logs\n",
    "\n",
    "\n",
    "# --- Initialization ---\n",
    "@st.cache_resource\n",
    "def init_agent():\n",
    "    repo_owner = \"DataTalksClub\"\n",
    "    repo_name = \"faq\"\n",
    "\n",
    "    def filter(doc):\n",
    "        return 'data-engineering' in doc['filename']\n",
    "\n",
    "    st.write(\"🔄 Indexing repo...\")\n",
    "    index = ingest.index_data(repo_owner, repo_name, filter=filter)\n",
    "    agent = search_agent.init_agent(index, repo_owner, repo_name)\n",
    "    return agent\n",
    "\n",
    "\n",
    "agent = init_agent()\n",
    "\n",
    "# --- Streamlit UI ---\n",
    "st.set_page_config(page_title=\"AI FAQ Assistant\", page_icon=\"🤖\", layout=\"centered\")\n",
    "st.title(\"🤖 AI FAQ Assistant\")\n",
    "st.caption(\"Ask me anything about the DataTalksClub/faq repository\")\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask your question...\"):\n",
    "    # User message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            response = asyncio.run(agent.run(user_prompt=prompt))\n",
    "            answer = rebsponse.output\n",
    "            st.markdown(answer)\n",
    "\n",
    "    # Save response to history + logs\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    logs.log_interaction_to_file(agent, response.new_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1a142-19b8-4e32-99b5-0c7ea652431a",
   "metadata": {},
   "source": [
    "**Running the basic Streamlit app**\n",
    "\n",
    "``uv run streamlit run app.py``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffad67-7f62-4257-a0d6-057aa4252ece",
   "metadata": {},
   "source": [
    "**app.py — Streaming version**\n",
    "\n",
    "This version streams the response word by word instead of showing everything at once. Feels much more responsive."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8cd24a4-dd1f-4f1f-aa69-38ad309df0b2",
   "metadata": {},
   "source": [
    "import streamlit as st\n",
    "import asyncio\n",
    "\n",
    "import ingest\n",
    "import search_agent\n",
    "import logs\n",
    "\n",
    "\n",
    "# --- Initialization ---\n",
    "@st.cache_resource\n",
    "def init_agent():\n",
    "    repo_owner = \"DataTalksClub\"\n",
    "    repo_name = \"faq\"\n",
    "\n",
    "    def filter(doc):\n",
    "        return \"data-engineering\" in doc[\"filename\"]\n",
    "\n",
    "    st.write(\"🔄 Indexing repo...\")\n",
    "    index = ingest.index_data(repo_owner, repo_name, filter=filter)\n",
    "    agent = search_agent.init_agent(index, repo_owner, repo_name)\n",
    "    return agent\n",
    "\n",
    "\n",
    "agent = init_agent()\n",
    "\n",
    "# --- Streamlit UI ---\n",
    "st.set_page_config(page_title=\"AI FAQ Assistant\", page_icon=\"🤖\", layout=\"centered\")\n",
    "st.title(\"🤖 AI FAQ Assistant\")\n",
    "st.caption(\"Ask me anything about the DataTalksClub/faq repository\")\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "\n",
    "# --- Streaming helper ---\n",
    "def stream_response(prompt: str):\n",
    "    async def agen():\n",
    "        async with agent.run_stream(user_prompt=prompt) as result:\n",
    "            last_len = 0\n",
    "            full_text = \"\"\n",
    "            async for chunk in result.stream_output(debounce_by=0.01):\n",
    "                # stream only the delta\n",
    "                new_text = chunk[last_len:]\n",
    "                last_len = len(chunk)\n",
    "                full_text = chunk\n",
    "                if new_text:\n",
    "                    yield new_text\n",
    "            # log once complete\n",
    "            logs.log_interaction_to_file(agent, result.new_messages())\n",
    "            st.session_state._last_response = full_text\n",
    "\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    agen_obj = agen()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            piece = loop.run_until_complete(agen_obj.__anext__())\n",
    "            yield piece\n",
    "    except StopAsyncIteration:\n",
    "        return\n",
    "\n",
    "\n",
    "# --- Chat input ---\n",
    "if prompt := st.chat_input(\"Ask your question...\"):\n",
    "    # User message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Assistant message (streamed)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response_text = st.write_stream(stream_response(prompt))\n",
    "\n",
    "    # Save full response to history\n",
    "    final_text = getattr(st.session_state, \"_last_response\", response_text)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227bc1b4-64e0-47e0-a91a-de19dfc6f998",
   "metadata": {},
   "source": [
    "#### **Deployment**\n",
    "\n",
    "**Export dependencies for Streamlit Cloud**\n",
    "\n",
    "Streamlit Cloud doesn't always work with uv directly, so export to requirements.txt:\n",
    "\n",
    "\n",
    "``uv export --no-dev > requirements.txt``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8fa86-3ef6-42fb-b9bd-d53883c7be1f",
   "metadata": {},
   "source": [
    "#### **Deploy to Streamlit Cloud**\n",
    "\n",
    "1. Push all your code to GitHub\n",
    "2. Go to share.streamlit.io\n",
    "3. Click \"New app\"\n",
    "4. Point it to your GitHub repo and the app.py file\n",
    "5. In the app settings, add your secret under \"Secrets\":\n",
    "\n",
    "``OPENAI_API_KEY=\"your-key\"``\n",
    "\n",
    "Your app is now live and anyone with the link can interact with your agent.\n",
    "\n",
    "That's all of Day 6. You went from a messy notebook to a clean, modular codebase with a web UI deployed on the internet. Day 7 covers wrapping up and sharing your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3105aa7-ebe2-4359-a027-de43733d2875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIHero Course",
   "language": "python",
   "name": "aihero-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
