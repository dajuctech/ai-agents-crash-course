{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d6f21a-21cc-4245-a6c9-9f133ba7a84a",
   "metadata": {},
   "source": [
    "### **Project: Credit Risk Scorecard AI Agent**\n",
    "\n",
    "**Repositories: ing-bank/skorecard, guillermo-navas-palencia/optbinning, levist7/Credit_Risk_Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b837cfb-6b3e-471e-aa42-f2c7a717d53f",
   "metadata": {},
   "source": [
    "#### **Day 1: Data Ingestion for Credit Risk Scorecard Docs**\n",
    "\n",
    "**Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a76aef5-1523-4664-98c3-35b2c1294d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a3751-e9bd-4655-a471-74023547c043",
   "metadata": {},
   "source": [
    "**Testing How Frontmatter Works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916dd6cf-e4f7-4b70-bec2-dd18b8722d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'title': 'Getting Started with AI', 'author': 'John Doe', 'tags': ['ai', 'machine-learning']}\n",
      "Content: # Hello World\n",
      "This is the content of the document.\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "tags: [\"ai\", \"machine-learning\"]\n",
    "---\n",
    "# Hello World\n",
    "This is the content of the document.\n",
    "\"\"\"\n",
    "\n",
    "post = frontmatter.loads(example)\n",
    "print(\"Metadata:\", dict(post))\n",
    "print(\"Content:\", post.content[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569c584-6f3f-4ec6-928d-d50c82f4ea93",
   "metadata": {},
   "source": [
    "**Downloading the Repo as a Zip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8005e98c-4ad4-4358-b220-b06bb00803fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing-bank/skorecard: Status 200\n",
      "guillermo-navas-palencia/optbinning: Status 200\n",
      "levist7/Credit_Risk_Modelling: Status 200\n"
     ]
    }
   ],
   "source": [
    "repos = [\n",
    "    ('ing-bank', 'skorecard', 'main'),\n",
    "    ('guillermo-navas-palencia', 'optbinning', 'master'),\n",
    "    ('levist7', 'Credit_Risk_Modelling', 'main'),\n",
    "]\n",
    "\n",
    "for owner, name, branch in repos:\n",
    "    url = f'https://codeload.github.com/{owner}/{name}/zip/refs/heads/{branch}'\n",
    "    resp = requests.get(url)\n",
    "    print(f\"{owner}/{name}: Status {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ee82d-bdaf-4dd0-b19c-d0a7930549b4",
   "metadata": {},
   "source": [
    "**Processing the Zip in Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15bf5ef-07e1-4fe3-b9af-a3fe95de19c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 44\n"
     ]
    }
   ],
   "source": [
    "repository_data = []\n",
    "\n",
    "url = 'https://codeload.github.com/ing-bank/skorecard/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if filename.endswith('.md') or filename.endswith('.mdx'):\n",
    "        with zf.open(file_info) as f_in:\n",
    "            content = f_in.read()\n",
    "            post = frontmatter.loads(content)\n",
    "            data = post.to_dict()\n",
    "            _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "            data['filename'] = filename_repo\n",
    "            repository_data.append(data)\n",
    "\n",
    "    elif filename.endswith('.ipynb'):\n",
    "        with zf.open(file_info) as f_in:\n",
    "            nb = json.loads(f_in.read())\n",
    "            text = ''\n",
    "            for cell in nb.get('cells', []):\n",
    "                source = ''.join(cell.get('source', []))\n",
    "                text += source + '\\n\\n'\n",
    "            _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "            data = {'content': text, 'filename': filename_repo}\n",
    "            repository_data.append(data)\n",
    "\n",
    "    elif filename.endswith('.rst'):\n",
    "        with zf.open(file_info) as f_in:\n",
    "            content = f_in.read().decode('utf-8', errors='ignore')\n",
    "            _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "            data = {'content': content, 'filename': filename_repo}\n",
    "            repository_data.append(data)\n",
    "\n",
    "zf.close()\n",
    "\n",
    "print(f\"Documents loaded: {len(repository_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f624930-c84a-4598-8f9c-16ae74e45d04",
   "metadata": {},
   "source": [
    "**Checking What We Got**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc6e47f-85e0-4d82-8d6b-85414259826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# Contributing guidelines\\n\\nMake sure to discuss any changes you would like to make in the issue board, before putting in any work.\\n\\n## Setup\\n\\nDevelopment install:\\n\\n```shell\\npip install -e \\'.[all]\\'\\n```\\n\\nUnit testing:\\n\\n```shell\\npytest\\n```\\n\\nWe use [pre-commit](https://pre-commit.com/) hooks to ensure code styling. Install with:\\n\\n```shell\\npre-commit install\\n```\\n\\nNow if you install it (which you are encouraged to do), you are encouraged to do the following command before committing your work:\\n\\n```shell\\npre-commit run --all-files\\n```\\n\\nThis will allow you to quickly see if the work you made contains some adaptions that you still might need to make before a pull request is accepted.\\n\\n## Documentation\\n\\nWe use [mkdocs](https://www.mkdocs.org) with [mkdocs-material](https://squidfunk.github.io/mkdocs-material/) theme. The docs are structured using the [divio documentation system](https://documentation.divio.com/). To view the docs locally:\\n\\n```shell\\npip install mkdocs-material\\nmkdocs serve\\n```\\n\\n## Releases and versioning\\n\\nWe use [semver](https://semver.org/) for versioning. When we are ready for a release, the maintainer runs:\\n\\n```shell\\ngit tag -a v0.1 -m \"skorecard v0.1\" && git push origin v0.1\\n```\\n\\nWhen we create a new github release a [github action](https://github.com/ing-bank/skorecard/blob/main/.github/workflows/publish_pypi.yml) is triggered that:\\n\\n- a new version will be deployed to pypi\\n- the docs will be re-built and deployed\\n\\n## Logo\\n\\n- We adapted the [\\'scores\\' noun](https://thenounproject.com/search/?q=score&i=1929515)\\n- We used [this color scheme](https://coolors.co/d7263d-f46036-2e294e-1b998b-c5d86d) from coolors.co\\n- We edited the logo using https://boxy-svg.com/app\\n\\n## Terminology\\n\\n- `BucketMapping` is a custom class that stores all the information needed for bucketing, including the map itself (either boundaries for binning, or a list of lists for categoricals)\\n- `FeaturesBucketMapping` is simply a collection of `BucketMapping`s, and is used to store all info for bucketing transformations for a dataset.', 'filename': 'CONTRIBUTING.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172305e9-4988-4fde-b74f-477a1e12a950",
   "metadata": {},
   "source": [
    "**Building a Reusable Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d408327d-4d8a-4c49-9252-346c268927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name, branch='main'):\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/{branch}'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename.lower()\n",
    "\n",
    "        if filename.endswith('.md') or filename.endswith('.mdx'):\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read()\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "                data['filename'] = filename_repo\n",
    "                repository_data.append(data)\n",
    "\n",
    "        elif filename.endswith('.ipynb'):\n",
    "            with zf.open(file_info) as f_in:\n",
    "                nb = json.loads(f_in.read())\n",
    "                text = ''\n",
    "                for cell in nb.get('cells', []):\n",
    "                    source = ''.join(cell.get('source', []))\n",
    "                    text += source + '\\n\\n'\n",
    "                _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "                data = {'content': text, 'filename': filename_repo}\n",
    "                repository_data.append(data)\n",
    "\n",
    "        elif filename.endswith('.rst'):\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                _, filename_repo = file_info.filename.split('/', maxsplit=1)\n",
    "                data = {'content': content, 'filename': filename_repo}\n",
    "                repository_data.append(data)\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035eb90-bd81-4e56-869f-d7616a37b867",
   "metadata": {},
   "source": [
    "**Pulling Data from All Three Repos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0fc80e3-312e-4ad9-99fd-f2c25e7a8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skorecard documents: 44\n",
      "optbinning documents: 44\n",
      "Credit Risk Modelling documents: 5\n",
      "\n",
      "Total documents: 93\n"
     ]
    }
   ],
   "source": [
    "skorecard_docs = read_repo_data('ing-bank', 'skorecard')\n",
    "optbinning_docs = read_repo_data('guillermo-navas-palencia', 'optbinning', branch='master')\n",
    "credit_risk_docs = read_repo_data('levist7', 'Credit_Risk_Modelling')\n",
    "\n",
    "print(f\"skorecard documents: {len(skorecard_docs)}\")\n",
    "print(f\"optbinning documents: {len(optbinning_docs)}\")\n",
    "print(f\"Credit Risk Modelling documents: {len(credit_risk_docs)}\")\n",
    "\n",
    "all_docs = skorecard_docs + optbinning_docs + credit_risk_docs\n",
    "print(f\"\\nTotal documents: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2075dc-8c4b-4332-ac8a-4376a314f2de",
   "metadata": {},
   "source": [
    "**Checking Document Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c59fdd-c431-42d7-a5be-10a162fa5f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGELOG.md.....................................................    607 chars\n",
      "CONTRIBUTING.md..................................................   2044 chars\n",
      "README.md........................................................   3780 chars\n",
      "docs/api/bucketers/AgglomerativeClusteringBucketer.md............     55 chars\n",
      "docs/api/bucketers/AsIsCategoricalBucketer.md....................     47 chars\n",
      "docs/api/bucketers/AsIsNumericalBucketer.md......................     45 chars\n",
      "docs/api/bucketers/DecisionTreeBucketer.md.......................     44 chars\n",
      "docs/api/bucketers/EqualFrequencyBucketer.md.....................     46 chars\n",
      "docs/api/bucketers/EqualWidthBucketer.md.........................     42 chars\n",
      "docs/api/bucketers/OptimalBucketer.md............................     39 chars\n",
      "docs/api/bucketers/OrdinalCategoricalBucketer.md.................     50 chars\n",
      "docs/api/bucketers/UserInputBucketer.md..........................     41 chars\n",
      "docs/api/datasets/load_uci_credit_card.md........................     55 chars\n",
      "docs/api/linear_model/LogisticRegression.md......................     45 chars\n",
      "docs/api/pipeline/BucketingProcess.md............................     39 chars\n"
     ]
    }
   ],
   "source": [
    "for doc in all_docs[:15]:\n",
    "    title = doc.get('description', doc['filename'])[:60]\n",
    "    print(f\"{title:.<65} {len(doc['content']):>6} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005c691-b2f9-40b8-a9a4-247862365f86",
   "metadata": {},
   "source": [
    "#### **Day 2: Chunking and Intelligent Processing for Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440ed1b-9170-488f-904a-f4f5215e1c13",
   "metadata": {},
   "source": [
    "**Sliding Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed96d59-7ea2-43a2-898c-c5886c114f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c43641-6e43-41c3-80f1-e8dc833a2792",
   "metadata": {},
   "source": [
    "**Test Sliding Window on One Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8943dff6-da4f-47ba-b29e-a722bd6a5c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 chunks\n"
     ]
    }
   ],
   "source": [
    "text = all_docs[1]['content']\n",
    "chunks = sliding_window(text, 2000, 1000)\n",
    "print(f\"Generated {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb6a2c-dd5c-43ae-ac0f-268e81a2db0b",
   "metadata": {},
   "source": [
    "**Apply Sliding Window to All Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22e997a-eeab-42e3-a789-eefb052f834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 399\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51261a8-af83-4b5a-bb86-7fc4de222c0b",
   "metadata": {},
   "source": [
    "**Section Splitting Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6322aa4-9b96-493d-8296-a494bb2dde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        header = parts[i] + parts[i+1]\n",
    "        header = header.strip()\n",
    "\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6196f-9e2f-4390-b446-495f6e83ae11",
   "metadata": {},
   "source": [
    "**Test Section Splitting on One Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e23723-cdf9-443e-910b-7a7d75da4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 5\n"
     ]
    }
   ],
   "source": [
    "text = all_docs[1]['content']\n",
    "sections = split_markdown_by_level(text, level=2)\n",
    "print(f\"Sections: {len(sections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c2937-652a-4baa-8c80-70efcd2a39ba",
   "metadata": {},
   "source": [
    "**Apply Section Splitting to All Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d5678e-13e8-4ead-b5bc-157324a1285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 122\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        all_chunks.append(section_doc)\n",
    "\n",
    "print(f\"Sections: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85953655-499a-44b2-ad1b-8b0bdc322d60",
   "metadata": {},
   "source": [
    "**Load API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f7594f-4e05-410e-adf3-ff19d9ec69f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env', override=True)\n",
    "print(\"Key loaded:\", \"OPENAI_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1403d9-adbf-4b32-883a-85e5b94b819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True\n",
      "Key ends with: 3IEA\n"
     ]
    }
   ],
   "source": [
    "print(\"Key loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Key ends with:\", os.environ.get('OPENAI_API_KEY', '')[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2949c05d-e31f-49ef-b3d5-a4c243a63bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d89e-8064-4ece-9717-b9edeca0d1b6",
   "metadata": {},
   "source": [
    "**OpenAI Client and LLM Helper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63cc3c07-382f-47f5-b9c2-9dd6118f8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5461ad-dbbb-4adc-a0a1-d424c7b39088",
   "metadata": {},
   "source": [
    "**Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d5cf2b-d311-4fef-b9a8-db21c04c375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22b4bf-fbae-4734-825f-be54c0168d96",
   "metadata": {},
   "source": [
    "**Intelligent Chunking Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eacb3e6e-451a-419c-be7c-b841170fc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892c0c0-9f38-4537-a688-9fada3eab48b",
   "metadata": {},
   "source": [
    "**Apply Intelligent Chunking to All Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d033dce-2438-44c1-a64b-e82f8c675c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea8600fa83b46a78235dede7e87a987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM chunks: 675\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for doc in tqdm(all_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        all_chunks.append(section_doc)\n",
    "\n",
    "print(f\"LLM chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e942d7e-7c33-48d1-98d9-4abd9485683f",
   "metadata": {},
   "source": [
    "**Compare All 3 Chunking Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc200c0-f0f1-4fc5-ac92-3e22ea2c961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: 2044 chars\n",
      "Title: CONTRIBUTING.md\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "doc = all_docs[1]\n",
    "text = doc['content']\n",
    "\n",
    "print(f\"Original document: {len(text)} chars\")\n",
    "print(f\"Title: {doc.get('description', doc['filename'])}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc324590-4361-4938-9f2c-e33735e039b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. SLIDING WINDOW: 2 chunks\n",
      "   Chunk 1 (2000 chars): # Contributing guidelines\n",
      "\n",
      "Make sure to discuss any changes you would like to ma...\n",
      "   Chunk 2 (1044 chars): # Releases and versioning\n",
      "\n",
      "We use [semver](https://semver.org/) for versioning. ...\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Sliding window\n",
    "sw_chunks = sliding_window(text, 2000, 1000)\n",
    "print(f\"\\n1. SLIDING WINDOW: {len(sw_chunks)} chunks\")\n",
    "for i, c in enumerate(sw_chunks[:3]):\n",
    "    print(f\"   Chunk {i+1} ({len(c['chunk'])} chars): {c['chunk'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5246a5-51fd-49bc-86a9-af995a1d026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. SECTION SPLITTING: 5 sections\n",
      "   Section 1 (551 chars): ## Setup\n",
      "   Section 2 (312 chars): ## Documentation\n",
      "   Section 3 (449 chars): ## Releases and versioning\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Section splitting\n",
    "sec_chunks = split_markdown_by_level(text, level=2)\n",
    "print(f\"\\n2. SECTION SPLITTING: {len(sec_chunks)} sections\")\n",
    "for i, s in enumerate(sec_chunks[:3]):\n",
    "    first_line = s.split('\\n')[0]\n",
    "    print(f\"   Section {i+1} ({len(s)} chars): {first_line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a77ee024-6d7c-4867-8645-e651571a77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: LLM chunking (only if API key works)\n",
    "# Uncomment below after fixing your API key:\n",
    "# llm_sec = intelligent_chunking(text)\n",
    "# print(f\"\\n3. LLM CHUNKING: {len(llm_sec)} sections\")\n",
    "# for i, s in enumerate(llm_sec[:3]):\n",
    "#     first_line = s.split('\\n')[0]\n",
    "#     print(f\"   Section {i+1} ({len(s)} chars): {first_line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f91dbc33-3cf1-46c0-9633-abb02be44382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Sliding window cuts text into fixed-size pieces with overlap.\n",
      "Good for unstructured text, but can split topics mid-sentence.\n",
      "\n",
      "Section splitting uses ## headers as natural boundaries.\n",
      "Each chunk covers one complete topic. Best for well-structured docs.\n",
      "\n",
      "LLM chunking lets AI decide where to split. Most accurate but\n",
      "costs money and is slow. Only needed for messy documents.\n",
      "\n",
      "For this project, SECTION SPLITTING is the best choice because\n",
      "the ML observability docs are well-structured with ## headers\n",
      "and each section covers one monitoring concept.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Sliding window cuts text into fixed-size pieces with overlap.\n",
    "Good for unstructured text, but can split topics mid-sentence.\n",
    "\n",
    "Section splitting uses ## headers as natural boundaries.\n",
    "Each chunk covers one complete topic. Best for well-structured docs.\n",
    "\n",
    "LLM chunking lets AI decide where to split. Most accurate but\n",
    "costs money and is slow. Only needed for messy documents.\n",
    "\n",
    "For this project, SECTION SPLITTING is the best choice because\n",
    "the ML observability docs are well-structured with ## headers\n",
    "and each section covers one monitoring concept.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0862ab-df94-49f8-86bb-03888f369bfe",
   "metadata": {},
   "source": [
    "#### **Day 3: Add Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f68bb-eb4c-48ab-9ed9-1ed75e219f1c",
   "metadata": {},
   "source": [
    "**Prepare Chunks for Search (if restarting kernel)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4c28c-c68c-4f6b-94eb-eaec00e67e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skorecard_docs = read_repo_data('ing-bank', 'skorecard')\n",
    "optbinning_docs = read_repo_data('guillermo-navas-palencia', 'optbinning', branch='master')\n",
    "credit_risk_docs = read_repo_data('levist7', 'Credit_Risk_Modelling')\n",
    "\n",
    "all_docs = skorecard_docs + optbinning_docs + credit_risk_docs\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a97f5b-4155-43af-ad0b-714ed7bf2087",
   "metadata": {},
   "source": [
    "**Text Search: Index the Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26c1c6-7c79-4d7b-973a-aee1742ae032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c0893-4ac2-45b2-bc1a-60e9dc665a5c",
   "metadata": {},
   "source": [
    "**Text Search: Test It**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6830f-d1d4-4cdb-bb44-f0d8655cef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do I create WoE bins for a credit feature?'\n",
    "results = index.search(query)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2830f9a-c4e1-48f3-8b86-7e1bf4234059",
   "metadata": {},
   "source": [
    "**Vector Search: Load the Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49745d0c-a315-4a95-9575-31749b3937bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31c23d-e281-4378-b8b6-76ad0dd907ab",
   "metadata": {},
   "source": [
    "**Vector Search: Test Embeddings and Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dc3cb-54f7-4a9c-90b9-93d1ea4c528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = all_chunks[2]\n",
    "text = record['chunk']\n",
    "v_doc = embedding_model.encode(text)\n",
    "\n",
    "query = 'What IV threshold should I use for feature selection?'\n",
    "v_query = embedding_model.encode(query)\n",
    "\n",
    "similarity = v_query.dot(v_doc)\n",
    "print(f\"Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d777624-6eed-4f59-be16-2c9a1bd1418f",
   "metadata": {},
   "source": [
    "**Vector Search: Create Embeddings for All Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e059b2e-f9dc-4db3-b67f-b312095e56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for d in tqdm(all_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    all_embeddings.append(v)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce17a5-9864-441a-b99a-4133c00af662",
   "metadata": {},
   "source": [
    "**Vector Search: Build the Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00859b06-6a62-4607-88c2-fd5cb5ad2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "credit_risk_vindex = VectorSearch()\n",
    "credit_risk_vindex.fit(all_embeddings, all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c645b-f96f-43dc-b395-4c2ce63a0ec3",
   "metadata": {},
   "source": [
    "**Vector Search: Test It**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe39d0b-6c40-444e-b4fd-da18ea6dd33b",
   "metadata": {},
   "outputs": [],
   "source": "query = 'How do I build a scorecard from logistic regression?'\nq = embedding_model.encode(query)\nresults = credit_risk_vindex.search(q)\nprint(results[0])"
  },
  {
   "cell_type": "markdown",
   "id": "82a0d93d-48bb-4c99-ba78-01f07745b3ce",
   "metadata": {},
   "source": [
    "**Hybrid Search: Combine Both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc871e-7adf-46ff-81d2-2fdb40729551",
   "metadata": {},
   "outputs": [],
   "source": "query = 'How do I calculate PSI for model monitoring?'\n\ntext_results = index.search(query, num_results=5)\n\nq = embedding_model.encode(query)\nvector_results = credit_risk_vindex.search(q, num_results=5)\n\nfinal_results = text_results + vector_results\nprint(f\"Total results: {len(final_results)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a47dbbac-00e9-4612-9031-80d1f12afcab",
   "metadata": {},
   "source": [
    "**Reusable Search Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff70f69-0644-4683-9ec9-81fd27370a12",
   "metadata": {},
   "outputs": [],
   "source": "def text_search(query):\n    return index.search(query, num_results=5)\n\ndef vector_search(query):\n    q = embedding_model.encode(query)\n    return credit_risk_vindex.search(q, num_results=5)\n\ndef hybrid_search(query):\n    text_results = text_search(query)\n    vector_results = vector_search(query)\n    \n    # Combine and deduplicate results\n    seen_ids = set()\n    combined_results = []\n\n    for result in text_results + vector_results:\n        if result['filename'] not in seen_ids:\n            seen_ids.add(result['filename'])\n            combined_results.append(result)\n    \n    return combined_results"
  },
  {
   "cell_type": "markdown",
   "id": "aa34d377-461f-4643-93c1-c45e6b8b67a9",
   "metadata": {},
   "source": [
    "**Test All Three Search Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f86cb-9b69-4f04-b7b0-19df155777ba",
   "metadata": {},
   "outputs": [],
   "source": "query = 'What IV threshold should I use for feature selection?'\n\nprint(\"Text search:\")\nprint(text_search(query)[0])\n\nprint(\"\\nVector search:\")\nprint(vector_search(query)[0])\n\nprint(\"\\nHybrid search:\")\nprint(hybrid_search(query)[0])"
  },
  {
   "cell_type": "markdown",
   "id": "a2b171c1-f710-44c3-a93b-794ab6245589",
   "metadata": {},
   "source": [
    "#### **Day 4: Agents and Tools**\n",
    "\n",
    "**LLM Without Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ce667-3576-45a6-949b-6658d691614c",
   "metadata": {},
   "outputs": [],
   "source": "import openai\n\nopenai_client = openai.OpenAI()\n\nuser_prompt = \"How do I create WoE bins for a credit feature?\"\n\nchat_messages = [\n    {\"role\": \"user\", \"content\": user_prompt}\n]\n\nresponse = openai_client.responses.create(\n    model='gpt-4o-mini',\n    input=chat_messages,\n)\n\nprint(response.output_text)"
  },
  {
   "cell_type": "markdown",
   "id": "852040d8-4d06-42a8-aac0-5831c80d9dfe",
   "metadata": {},
   "source": [
    "**Define the Search Function for OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a2163da-3032-49bd-9465-98fc7674d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302739c-3390-4cee-bbd9-7dd79d9a1aba",
   "metadata": {},
   "source": [
    "**Describe the Function for OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b897b7-a299-44e1-9e99-33281dbdd923",
   "metadata": {},
   "outputs": [],
   "source": "text_search_tool = {\n    \"type\": \"function\",\n    \"name\": \"text_search\",\n    \"description\": \"Search the credit risk scorecard knowledge base\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search query text to look up in the credit risk docs.\"\n            }\n        },\n        \"required\": [\"query\"],\n        \"additionalProperties\": False\n    }\n}"
  },
  {
   "cell_type": "markdown",
   "id": "f192f0a0-db9c-4de7-97c7-62edf9c113b2",
   "metadata": {},
   "source": [
    "**Send Question with Tool Available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9eb5f6-7174-4e5e-8487-453238768380",
   "metadata": {},
   "outputs": [],
   "source": "system_prompt = \"\"\"\nYou are a helpful assistant for credit risk scorecard development. \n\"\"\"\n\nquestion = \"How do I create WoE bins for a credit feature?\"\n\nchat_messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": question}\n]\n\nresponse = openai_client.responses.create(\n    model='gpt-4o-mini',\n    input=chat_messages,\n    tools=[text_search_tool]\n)"
  },
  {
   "cell_type": "markdown",
   "id": "46d92fc9-cb7b-4b62-9e9d-a476f294e232",
   "metadata": {},
   "source": [
    "**Inspect the Tool Call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f61eb8f-fa91-4a8e-b176-2eb4c1934eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseOutputMessage(id='msg_0303bac07097203500698e594243188194974273f6846ab100', content=[ResponseOutputText(annotations=[], text='To detect data drift in your credit scoring model, you can follow these key strategies:\\n\\n### 1. Understand Types of Drift\\n- **Feature Drift**: Changes in the input feature distributions over time.\\n- **Prediction Drift**: Changes in the distribution of model outputs (predictions) over time.\\n\\n### 2. Set Up Monitoring\\nMonitor both feature and prediction drift, as they can provide early warnings about potential model quality decay.\\n\\n### 3. Choose Detection Methods\\nSelect appropriate methods for detecting drift:\\n- **Statistical Tests**: \\n  - **Kolmogorov-Smirnov Test**: Useful for numerical features.\\n  - **Chi-squared Test**: Useful for categorical features.\\n- **Distance Metrics**:\\n  - **Wasserstein Distance**: Effective for numerical data in larger datasets.\\n  - **Jensen-Shannon Divergence**: Suitable for categorical features.\\n- **Rule-Based Checks**: Custom rules based on domain knowledge that account for anticipated changes.\\n\\n### 4. Define Drift Detection Conditions\\nSet thresholds for drift detection:\\n- **Drift Detection Threshold**: Establish numeric thresholds or confidence levels for your chosen methods.\\n- **Reference Dataset**: Use a well-defined historical dataset to compare distributions.\\n\\n### 5. Monitor Data Quality\\nEnsure data quality metrics are calculated first, as poor data quality can masquerade as data drift.\\n\\n### 6. Consider Univariate vs. Multivariate Drift\\n- **Univariate Drift**: Analyze each feature individually for drift.\\n- **Multivariate Drift**: Look at the complete dataset, which can help identify complex interactions.\\n\\n### 7. Segment-Based Monitoring\\nFor diverse datasets, consider monitoring drift within clearly defined segments (e.g., customer demographics).\\n\\n### General Tips\\n- Prediction drift is often more critical than feature drift; prioritize monitoring model outputs.\\n- Not all distribution drift results in model performance decay; assess the context and significance of detected drifts.\\n- Even if you have labels, monitor feature drift, as its emergence might precede observable model decay.\\n\\nThis multi-faceted approach will help you effectively monitor and detect data drift in your credit scoring model, allowing for proactive strategies to maintain model performance.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n"
     ]
    }
   ],
   "source": [
    "print(response.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b917e8-18b4-4bff-b3da-536dfbf1ebb8",
   "metadata": {},
   "outputs": [],
   "source": "system_prompt = \"\"\"\nYou are a helpful assistant for credit risk scorecard development. \nAlways use the search tool before answering any question. Never answer without searching first.\n\"\"\"\n\nquestion = \"How do I create WoE bins for a credit feature?\"\n\nchat_messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": question}\n]\n\nresponse = openai_client.responses.create(\n    model='gpt-4o-mini',\n    input=chat_messages,\n    tools=[text_search_tool]\n)\n\nprint(response.output)"
  },
  {
   "cell_type": "markdown",
   "id": "09259979-04ae-4d33-8d63-0d60d931f991",
   "metadata": {},
   "source": [
    "**Execute Tool Call and Send Results Back**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5302ed2a-88ea-43fd-acb4-5c588f5268a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To detect data drift in your credit scoring model, consider the following steps:\n",
      "\n",
      "1. **Monitor Data Drift**: Track changes in the input feature distributions. This involves comparing the statistical properties of the current input data to those of a baseline dataset (the training data, for example).\n",
      "\n",
      "2. **Output Drift Detection**: Observe shifts in model predictions over time. If the model's predictions change significantly without corresponding changes in the input data, it may indicate issues with model performance.\n",
      "\n",
      "3. **Statistical Methods**: Use statistical tests or distance metrics to assess distribution drift. Common methods include:\n",
      "   - Kolmogorov-Smirnov tests for univariate features.\n",
      "   - Chi-squared tests for categorical variables.\n",
      "   - Kullback-Leibler divergence for comparing distributions.\n",
      "\n",
      "4. **Set Thresholds**: Define what constitutes significant drift using acceptable thresholds. These could be based on confidence levels or specific numeric thresholds.\n",
      "\n",
      "5. **Segmentation**: If your data comprises distinct segments (e.g., different demographic groups), monitor drift separately for each segment to gain more granular insights.\n",
      "\n",
      "6. **Alerts and Monitoring**: Implement alert conditions to notify you of significant drift, which may prompt further action or investigation.\n",
      "\n",
      "7. **Process for Updates**: Regularly review and update your monitoring processes to adapt to new trends or changes in your data landscape.\n",
      "\n",
      "By tracking both data drift and prediction drift, you can anticipate potential problems before they affect your model's performance. This proactive approach will help maintain the reliability and accuracy of your credit scoring model.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n",
    "\n",
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbb47f-a31b-4ed6-9246-56be857fc2d9",
   "metadata": {},
   "source": [
    "**Better System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a987c9-f655-4131-b3d3-dd876152236e",
   "metadata": {},
   "outputs": [],
   "source": "system_prompt = \"\"\"\nYou are a helpful assistant for credit risk scorecard development. \n\nUse the search tool to find relevant information from the credit risk and scorecard materials before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.\n\"\"\""
  },
  {
   "cell_type": "markdown",
   "id": "556267f3-1e96-44fc-a7ff-bcf203e83a8e",
   "metadata": {},
   "source": [
    "**Encouraging Multiple Searches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560799b5-2c4c-4007-9faa-b2c16cb13560",
   "metadata": {},
   "outputs": [],
   "source": "system_prompt = \"\"\"\nYou are a helpful assistant for credit risk scorecard development. \n\nAlways search for relevant information before answering. \nIf the first search doesn't give you enough information, try different search terms.\n\nMake multiple searches if needed to provide comprehensive answers.\n\"\"\""
  },
  {
   "cell_type": "markdown",
   "id": "05d0f310-4332-46e0-8050-c5a543644743",
   "metadata": {},
   "source": [
    "**Pydantic AI: Redefine Search with Type Hints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619da8e7-b4b8-4b3a-81ab-5b18d2169e66",
   "metadata": {},
   "outputs": [],
   "source": "from typing import List, Any\n\ndef text_search(query: str) -> List[Any]:\n    \"\"\"\n    Perform a text-based search on the credit risk scorecard index.\n\n    Args:\n        query (str): The search query string.\n\n    Returns:\n        List[Any]: A list of up to 5 search results returned by the index.\n    \"\"\"\n    return index.search(query, num_results=5)"
  },
  {
   "cell_type": "markdown",
   "id": "aeec3104-2199-4c53-b681-34b987dc4f39",
   "metadata": {},
   "source": [
    "**Create the Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c800117-816d-40f9-9884-d200dc86e292",
   "metadata": {},
   "outputs": [],
   "source": "from pydantic_ai import Agent\n\nagent = Agent(\n    name=\"credit_risk_agent\",\n    instructions=system_prompt,\n    tools=[text_search],\n    model='gpt-4o-mini'\n)"
  },
  {
   "cell_type": "markdown",
   "id": "b2d38e0e-70bc-4d4a-8c57-b28fdb7b57e9",
   "metadata": {},
   "source": [
    "**Run the Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886440d-bfb1-4422-b496-d2637a65a8e4",
   "metadata": {},
   "outputs": [],
   "source": "question = \"How do I create WoE bins for a credit feature?\"\n\nresult = await agent.run(user_prompt=question)\nprint(result.output)"
  },
  {
   "cell_type": "markdown",
   "id": "8c5b5bd1-f09f-45ff-9813-316e4c20ec5e",
   "metadata": {},
   "source": [
    "**Look Inside the Agent's Reasoning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf5303-a18c-4262-9abd-67a9498188b4",
   "metadata": {},
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ea966-50f6-4245-9542-2b741279147f",
   "metadata": {},
   "source": [
    "**Update System Prompt with Citations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af023233-24f3-45d4-9d46-89a246e54fa2",
   "metadata": {},
   "outputs": [],
   "source": "system_prompt = \"\"\"\nYou are a helpful assistant for credit risk scorecard development.  \n\nUse the search tool to find relevant information from the credit risk and scorecard materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, use the full path to the GitHub repository for the relevant repo:\n- skorecard docs: \"https://github.com/ing-bank/skorecard/blob/main/\"\n- optbinning docs: \"https://github.com/guillermo-navas-palencia/optbinning/blob/master/\"\n- Credit Risk Modelling docs: \"https://github.com/levist7/Credit_Risk_Modelling/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.  \n\"\"\".strip()\n\nagent = Agent(\n    name=\"credit_risk_agent_v2\",\n    instructions=system_prompt,\n    tools=[text_search],\n    model='gpt-4o-mini'\n)"
  },
  {
   "cell_type": "markdown",
   "id": "5aecf68c-921e-4bd2-a359-f73d29b60d15",
   "metadata": {},
   "source": [
    "**Test the Updated Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e08a55-aa32-4a6b-8cdb-c0397915e79a",
   "metadata": {},
   "outputs": [],
   "source": "question = \"What IV threshold should I use for feature selection?\"\n\nresult = await agent.run(user_prompt=question)\nprint(result.output)"
  },
  {
   "cell_type": "markdown",
   "id": "bab35b98-a188-4633-9a90-82c289e07aec",
   "metadata": {},
   "source": [
    "#### **Day 5: Evaluation**\n",
    "\n",
    "**Recap: Set Up Agent (if kernel restarted)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc31da-f4a5-410b-a27a-7e61265134bc",
   "metadata": {},
   "outputs": [],
   "source": "from typing import List, Any\nfrom pydantic_ai import Agent\n\n\ndef text_search(query: str) -> List[Any]:\n    \"\"\"\n    Perform a text-based search on the credit risk scorecard index.\n\n    Args:\n        query (str): The search query string.\n\n    Returns:\n        List[Any]: A list of up to 5 search results returned by the index.\n    \"\"\"\n    return index.search(query, num_results=5)\n\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant for credit risk scorecard development.  \n\nUse the search tool to find relevant information from the credit risk and scorecard materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, use the full path to the GitHub repository for the relevant repo:\n- skorecard docs: \"https://github.com/ing-bank/skorecard/blob/main/\"\n- optbinning docs: \"https://github.com/guillermo-navas-palencia/optbinning/blob/master/\"\n- Credit Risk Modelling docs: \"https://github.com/levist7/Credit_Risk_Modelling/blob/main/\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.  \n\"\"\".strip()\n\nagent = Agent(\n    name=\"credit_risk_agent_v2\",\n    instructions=system_prompt,\n    tools=[text_search],\n    model='gpt-4o-mini'\n)"
  },
  {
   "cell_type": "markdown",
   "id": "9afc6c68-5f0e-48b7-98b0-b87654308b0f",
   "metadata": {},
   "source": [
    "**Test the Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6085c2c-112e-48c7-bdd0-6f725f8c1603",
   "metadata": {},
   "outputs": [],
   "source": "question = \"How do I calculate PSI for model monitoring?\"\nresult = await agent.run(user_prompt=question)\nprint(result.output)"
  },
  {
   "cell_type": "markdown",
   "id": "d93cb0c0-541e-49d3-b97e-aeba9c7af4f9",
   "metadata": {},
   "source": [
    "**Build the Log Entry Extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1efc22c6-b6d3-48d0-bc93-46b5462b2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c8c2c-85ea-4a63-b5d7-0359c3c2a1af",
   "metadata": {},
   "source": [
    "**Write Logs to Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbcb6ad2-d53e-43a7-8bfe-b77b631c6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    \n",
    "    if isinstance(ts, str):\n",
    "        ts_obj = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    else:\n",
    "        ts_obj = ts\n",
    "    \n",
    "    ts_str = ts_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e38a42-c5d9-45e5-b7ed-0cfdc2a5fc98",
   "metadata": {},
   "source": [
    "**Interactive Vibe Checking with Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1dba82f4-1033-4b3a-8d78-35f820d73d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is history\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is the study of past events, particularly in human affairs. It encompasses various aspects such as political, social, economic, and cultural developments over time. Historians analyze written documents, oral accounts, and other forms of evidence to understand and interpret the events that have shaped societies and civilizations. The study of history helps people to understand how the past influences the present and future, providing context for current affairs and societal trends.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/ml_monitoring_agent_v2_20260212_230246_248a7f.json')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d2e1b-a9bc-4ca9-992d-b620a863e154",
   "metadata": {},
   "source": [
    "**Evaluation Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52fd637c-1eca-4135-a845-3e521d4d7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509bc7a-c149-49fa-95d9-b3c4d0dd50e4",
   "metadata": {},
   "source": [
    "**Structured Output Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4437b107-5795-44b8-88bb-6f270de34de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac873347-0085-4ddc-87e7-6cfc58089c40",
   "metadata": {},
   "source": [
    "**Create the Evaluation Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b0e6205-c892-4968-81be-cca12ac66f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-4o-mini',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2a6a0-26d3-4f82-83f0-b0fd8cf157d7",
   "metadata": {},
   "source": [
    "**Evaluation Input Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d833d908-140e-452c-9a9f-5ac1aab08a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55419a6-bcfd-4a70-a79c-9d38a9b176f7",
   "metadata": {},
   "source": [
    "**Load Log Files Helper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1845be20-8203-4c40-8e60-39b11a06eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e2947-a019-4ebf-aaa3-69a8e64b32d3",
   "metadata": {},
   "source": [
    "**Check What Log Files Exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cead8448-a5b8-46e8-b970-26730777bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech_interview_agent_20260122_131205_432929.json\n",
      "ml_monitoring_agent_v2_20260212_230246_248a7f.json\n",
      "tech_interview_agent_20260121_202846_16a492.json\n",
      "tech_interview_agent_20260122_142625_dc4ce6.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in os.listdir('./logs'):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e37fda-bd4a-4cca-a3a5-ed9d29d3e508",
   "metadata": {},
   "source": [
    "**Run Evaluation on a Single Log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85518d1-6c81-4cad-bcad-1ff3b0a56154",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nlog_files = sorted(LOG_DIR.glob('*.json'))\nif log_files:\n    log_record = load_log_file(log_files[0])\n\n    instructions = log_record['system_prompt']\n    question = log_record['messages'][0]['parts'][0]['content']\n    answer = log_record['messages'][-1]['parts'][0]['content']\n    log = json.dumps(log_record['messages'])\n\n    user_prompt = user_prompt_format.format(\n        instructions=instructions,\n        question=question,\n        answer=answer,\n        log=log\n    )\n\n    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n\n    checklist = result.output\n    print(checklist.summary)\n\n    for check in checklist.checklist:\n        print(check)\nelse:\n    print(\"No log files found. Run the agent first to generate logs.\")"
  },
  {
   "cell_type": "markdown",
   "id": "654510a1-19f8-4bec-9401-d8b06bded8cf",
   "metadata": {},
   "source": [
    "**Simplify Log Messages to Save Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a65d1bba-f3b9-49cd-91dd-372435f339c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                part.pop('timestamp', None)\n",
    "            if kind == 'tool-call':\n",
    "                part.pop('tool_call_id', None)\n",
    "            if kind == 'tool-return':\n",
    "                part.pop('tool_call_id', None)\n",
    "                part.pop('metadata', None)\n",
    "                part.pop('timestamp', None)\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                part.pop('id', None)\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8a609-12ec-424e-a29e-5583d4c5081e",
   "metadata": {},
   "source": [
    "**Combined Evaluation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ace81e3c-cb11-4d02-9b70-40bc0f8b9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9f649-934b-4b48-8419-e24deb05c73f",
   "metadata": {},
   "source": [
    "**Test the Combined Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334620db-6770-4cbb-bc5b-bd4603b7e18f",
   "metadata": {},
   "outputs": [],
   "source": "log_files = sorted(LOG_DIR.glob('*.json'))\nif log_files:\n    log_record = load_log_file(log_files[-1])\n    eval1 = await evaluate_log_record(eval_agent, log_record)\nelse:\n    print(\"No log files found. Run the agent first to generate logs.\")"
  },
  {
   "cell_type": "markdown",
   "id": "66579e45-4e01-419d-8436-2234c3a03142",
   "metadata": {},
   "source": [
    "**View the Evaluation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d5d86907-5d1e-4e03-9c0c-c22e64ee79cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AI agent effectively followed the user's instructions, provided a relevant and clear answer, included the necessary citation, and thoroughly covered all relevant points regarding preparation for an AI Engineer course.\n",
      "check_name='instructions_follow' justification='The agent provided a comprehensive answer aligned with the instructions, including recommendations for AI preparation.' check_pass=True\n",
      "check_name='instructions_avoid' justification='The agent did not include any prohibited actions as per the instructions.' check_pass=True\n",
      "check_name='answer_relevant' justification=\"The answer directly addresses the user's question about preparing for an AI Engineer course.\" check_pass=True\n",
      "check_name='answer_clear' justification='The answer is structured in a clear and concise manner, using bullet points and headings for easy reading.' check_pass=True\n",
      "check_name='answer_citations' justification='The agent included a citation to the Tech Interview Handbook as required, referencing its GitHub link.' check_pass=True\n",
      "check_name='completeness' justification='The answer covers all key areas necessary for preparation, such as fundamentals, programming skills, and practical experience.' check_pass=True\n",
      "check_name='tool_call_search' justification='The agent invoked the search tool to find relevant information before answering, adhering to the instructions.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "print(eval1.summary)\n",
    "\n",
    "for check in eval1.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacebb3-225e-4c37-8480-d10f4d2958fd",
   "metadata": {},
   "source": [
    "**Question Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b6ba1-f983-48b4-9e41-a87ed648727c",
   "metadata": {},
   "outputs": [],
   "source": "question_generation_prompt = \"\"\"\nYou are helping to create test questions for an AI agent that answers questions about credit risk scorecard development.\n\nBased on the provided course content, generate realistic questions that practitioners might ask.\n\nThe questions should:\n\n- Be natural and varied in style\n- Range from simple to complex\n- Include both specific technical questions and general credit risk questions\n\nGenerate one question for each record.\n\"\"\".strip()\n\nclass QuestionsList(BaseModel):\n    questions: list[str]\n\nquestion_generator = Agent(\n    name=\"question_generator\",\n    instructions=question_generation_prompt,\n    model='gpt-4o-mini',\n    output_type=QuestionsList\n)"
  },
  {
   "cell_type": "markdown",
   "id": "8b310c6b-9f62-4e28-a4c9-6e20f49a0c00",
   "metadata": {},
   "source": [
    "**Generate Questions from Sampled Records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3ee53-1f89-4bd2-aae3-c4dc30b50264",
   "metadata": {},
   "outputs": [],
   "source": "import random\n\nsample = random.sample(all_docs, 10)\nprompt_docs = [d['content'] for d in sample]\nprompt = json.dumps(prompt_docs)\n\nresult = await question_generator.run(prompt)\nquestions = result.output.questions"
  },
  {
   "cell_type": "markdown",
   "id": "ea2eb948-c71c-4822-a882-9a225f410e6f",
   "metadata": {},
   "source": [
    "**Run Agent on Generated Questions and Log Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf70e706-53f9-49cf-84ce-cb283a7cbbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10618b19fa524dd9815e12c7cd866e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the key metrics to prioritize for effective ML monitoring?\n",
      "To effectively monitor machine learning (ML) models, it's essential to prioritize the right metrics. Here are the key categories and examples of metrics to consider:\n",
      "\n",
      "1. **Symptoms (Key Metrics)**: These are essential for identifying issues quickly and are typically used for alerting. They include metrics like accuracy and business KPIs (Key Performance Indicators). Examples:\n",
      "   - 1-day accuracy.\n",
      "   - KPIs related to business objectives.\n",
      "\n",
      "2. **Summary Metrics**: These provide extra context to understand the performance of the model better. Examples include:\n",
      "   - Prediction volume.\n",
      "   - True positives and true negatives.\n",
      "   - Share of missing data.\n",
      "\n",
      "3. **Performance Profiling**: This involves a more detailed examination of model performance and outputs, such as:\n",
      "   - Per-column descriptive statistics.\n",
      "   - Distributions of outputs.\n",
      "   - Segmented model performance information.\n",
      "\n",
      "4. **Debugging and Analytics Data**: Metrics that help in deeper analysis of model performance. Examples:\n",
      "   - Correlations.\n",
      "   - Metrics for explainability.\n",
      "\n",
      "5. **Raw Data Prediction Logs**: This data can be utilized to calculate custom metrics or create visualizations, providing a comprehensive view of model performance.\n",
      "\n",
      "### Comprehensive Monitoring Metrics\n",
      "Depending on the complexity of the model's use case, more advanced metrics may include:\n",
      "- **Model Bias and Fairness**: Essential for sensitive domains like healthcare.\n",
      "- **Outliers**: Important to monitor especially where errors may incur high costs.\n",
      "- **Explainability Metrics**: These are crucial for user trust and understanding of model decisions.\n",
      "\n",
      "### Key Practices\n",
      "- Always monitor service health metrics.\n",
      "- Store prediction logs that capture inputs and outputs.\n",
      "- Limit alerts to crucial metrics to avoid false alerts.\n",
      "- Define the depth of monitoring based on specific needs of the project.\n",
      "\n",
      "In conclusion, a balanced approach is recommended where you select several metric groups (e.g., model quality, data quality, data drift) and define key signals for each group to derive maximum insights from monitoring systems.\n",
      "\n",
      "For a deeper dive into effective ML monitoring metrics, you can refer to the learning material on [How to prioritize ML monitoring metrics](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/how-to-prioritize-monitoring-metrics.md) and [Custom metrics in ML monitoring](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/custom-metrics-ml-monitoring.md).\n",
      "\n",
      "How can additional context be logged for debugging and analysis in model monitoring?\n",
      "To log additional context for debugging and analysis in model monitoring, you can follow a structured approach to implement effective logging practices:\n",
      "\n",
      "1. **Capture Service Logs**: It's essential to log various service events, particularly prediction events, which occur whenever the model receives input data and returns output. This foundational step helps in monitoring and debugging the health of your service.\n",
      "\n",
      "2. **Capture Prediction Logs**: When logging prediction events, ensure that you include:\n",
      "   - Model input data\n",
      "   - Model output\n",
      "   - Ground truth (if available)\n",
      "   \n",
      "   These prediction logs are crucial for monitoring the quality of the ML model and are also necessary for model retraining, debugging, and audits.\n",
      "\n",
      "3. **Log ML Monitoring Metrics**: The architecture for logging will depend on your model deployment. Typically, it includes a **prediction store** that records prediction data, requiring long-term secure storage and backup features. Your ML monitoring implementation can either pull data from this prediction store or use a pipeline manager to compute the necessary monitoring metrics.\n",
      "\n",
      "4. **Utilize Monitoring Dashboards**: After logging the necessary data, implement a metric store to store and retrieve computed metrics efficiently. A monitoring dashboard will then visualize these metrics, having a direct connection to the metric store for timely insights.\n",
      "\n",
      "5. **Consider Environmental Context**: Capture context regarding the environment stability, feedback loops, and model criticality. This contextual information can significantly enhance your debugging capabilities and speed up issue resolution.\n",
      "\n",
      "These steps will create a comprehensive logging strategy that aids in debugging and analysis, thereby improving the overall robustness of your ML monitoring system. \n",
      "\n",
      "For more detail on the implementation of logging in ML monitoring, refer to the course materials: [Logging for ML monitoring](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/logging-ml-monitoring.md).\n",
      "\n",
      "What is the difference between direct and proxy metrics in monitoring model performance?\n",
      "In the context of monitoring model performance, the key difference between **direct** and **proxy metrics** lies in their measurement and purpose:\n",
      "\n",
      "1. **Direct Metrics**:\n",
      "   - These metrics provide a straightforward measure of model quality and performance outcomes. Examples include accuracy, root mean square error (RMSE), and other business key performance indicators (KPIs) that can be directly observed or calculated from the model's predictions against ground truth.\n",
      "   - They are preferred when accurate ground truth data is available since they directly reflect the model's effectiveness and alignment with the desired objectives. \n",
      "\n",
      "2. **Proxy Metrics**:\n",
      "   - Proxy metrics are used when direct measurements are not feasible or when the relevant ground truth data is unavailable. They rely on heuristics or indirect indicators that represent aspects of model performance.\n",
      "   - Examples of proxy metrics include monitoring output distribution drift, compliance with output ranges, or evaluating model predictions based on historical patterns or domain knowledge.\n",
      "   - Essentially, they serve as substitutes that approximatively reflect model performance and are useful for diagnosing issues or estimating impacts when direct assessment is impossible.\n",
      "\n",
      "It's essential to choose metrics carefully, ensuring they are actionable and reflect meaningful performance thresholds that can trigger responses when necessary [source](https://github.com/evidentlyai/ml_observability_course/blob/main/ml_observability_course-main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/how-to-prioritize-monitoring-metrics.md).\n",
      "\n",
      "In what situations would minimalistic monitoring be appropriate for ML models?\n",
      "Minimalistic monitoring for ML models is appropriate in various contexts where the stakes are lower, and the operational complexity is manageable. Here are some key situations where minimalistic monitoring can be effective:\n",
      "\n",
      "1. **Fewer Models in Production**: If you have just a few ML models running, it may not be necessary to implement comprehensive monitoring systems. Minimalistic approaches can suffice to keep track of their performance.\n",
      "\n",
      "2. **Smaller Datasets**: When dealing with small datasets, the overhead of extensive monitoring might not be justified. Minimalistic monitoring allows for effective oversight without being resource-intensive.\n",
      "\n",
      "3. **Less Critical Use Cases**: For applications where the consequences of model failure are not severe, such as low-impact business decisions or internal tools, a lighter monitoring framework is often sufficient.\n",
      "\n",
      "4. **Technical Users**: If the monitoring system is primarily for technical users who are comfortable investigating issues on their own, a minimalistic approach that logs less data and focuses on only a few key metrics can be appropriate.\n",
      "\n",
      "5. **Easy Access to Predictions**: If predictions can be easily accessed for further analysis, there's less need to maintain extensive monitoring logs since issues can be investigated as they arise.\n",
      "\n",
      "6. **Low Prediction Frequency**: When predictions are made infrequently, the complexities of maintaining a robust monitoring system may not be cost-effective, allowing minimalistic monitoring to be a viable option.\n",
      "\n",
      "In contrast, more in-depth monitoring becomes necessary when there are multiple models, larger datasets, critical use cases with high costs for errors, or when the monitoring needs to cater to business stakeholders [source: ml_observability_course-main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/how-to-prioritize-monitoring-metrics.md].\n",
      "\n",
      "How do you determine the best reference dataset for detecting drift in production?\n",
      "To determine the best reference dataset for detecting drift in production, it's important to consider several factors that contribute to the effectiveness of the dataset. Here are key characteristics and considerations to guide your selection:\n",
      "\n",
      "### Characteristics of a Good Reference Dataset\n",
      "1. **Realistic Data Patterns**: The dataset should reflect realistic data distributions, including any cycles and seasonality that may influence the data.\n",
      "2. **Sample Size**: It should contain a large enough sample to provide meaningful statistical analysis.\n",
      "3. **Typical Scenarios**: It should encapsulate realistic scenarios (e.g., sensor outages, peak times) to help validate against new, incoming data.\n",
      "\n",
      "### Types of Reference Datasets\n",
      "- **Hold-out Validation Data**: This is preferred over training data for creating reference datasets, particularly for drift detection, as it more accurately represents the data you expect in production.\n",
      "- **Historical Data**: Analyzing historical datasets can provide insights into seasonality and trends that are critical for understanding what typical distributions look like.\n",
      "\n",
      "### Considerations for Comparison\n",
      "- **Update Frequency**: Decide whether the reference dataset will be static (e.g., updated monthly) or dynamic (e.g., using a sliding window approach).\n",
      "- **Batch Size**: Determine whether you want to compare daily, weekly, or monthly data batches when assessing drift.\n",
      "- **Use of Multiple References**: It may be beneficial to use multiple reference datasets to capture variations over time and during different scenarios.\n",
      "\n",
      "### Avoid Common Mistakes\n",
      "- Training data is generally not recommended as a reference dataset because it may introduce biases from preprocessing. Use training data only when there's no other option.\n",
      "- A reference dataset should not be mistaken for a \"golden dataset,\" as its purpose is different (the latter being a perfect dataset for all checks).\n",
      "\n",
      "In summary, choosing a reference dataset is about tailoring it to the specific context of your application, accounting for the typical data distributions and any periodic variations that could affect model predictions. Historical data plays an integral role in informing your choice, ensuring it accurately captures relevant data characteristics over time. \n",
      "\n",
      "For more detailed information, you can refer to the course material on choosing a reference dataset here: [How to choose a reference dataset in ML monitoring](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/how-to-choose-reference-dataset-ml-monitoring.md).\n",
      "\n",
      "What are some strategies for monitoring embedding drift in high-dimensional data?\n",
      "When monitoring embedding drift in high-dimensional data, there are several effective strategies that can be employed:\n",
      "\n",
      "1. **Distance Metrics**: You can use distance metrics to assess shifts in the embeddings by measuring the distances between vectors. Common options include:\n",
      "   - **Euclidean Distance**: Measures the straight-line distance between points in the embedding space.\n",
      "   - **Cosine Distance**: Assesses the angle between vectors, which is useful for understanding how similar or dissimilar the embeddings are.\n",
      "   By calculating the distance between centroids of reference data and current data, shifts can be detected.\n",
      "\n",
      "2. **Model-based Drift Detection**: This method involves using the embeddings to train a domain classifier that distinguishes between the reference data and current data. This approach is akin to model-based drift detection in raw data but comes with the limitation that it does not indicate which features or components are responsible for the drift.\n",
      "\n",
      "3. **Share of Drifted Components**: This strategy involves treating each component of the embeddings independently. For each embedding component, a drift score can be assessed, and these individual scores can be aggregated to determine the total number of drifted components or the share of drifted components.\n",
      "\n",
      "These methods provide various avenues for detecting and understanding embedding drift, helping ensure that model performance remains robust over time. For a deeper exploration, you can consult the complete details in the following source: [Monitoring embeddings drift](https://github.com/evidentlyai/ml_observability_course/blob/main/ml_observability_course-main/docs/book/ml-observability-course/module-3-ml-monitoring-for-unstructured-data/monitoring-embeddings-drift.md).\n",
      "\n",
      "Can you explain how to implement custom metrics with the Evidently library?\n",
      "To implement custom metrics using the Evidently library, you can follow these key steps:\n",
      "\n",
      "### 1. Understand the Need for Custom Metrics\n",
      "Custom metrics are often necessary to capture specific aspects of model performance that standard metrics may not fully address. This could include aligning metrics with business objectives or reflecting domain-specific requirements.\n",
      "\n",
      "### 2. Types of Custom Metrics\n",
      "You can categorize custom metrics into different types:\n",
      "\n",
      "- **Business and Product KPIs:** Metrics aligned with business goals, such as:\n",
      "  - Number of successful chatbot completions.\n",
      "  - Fraud detection metrics, like the count of detected fraud cases over $50,000.\n",
      "  - Performance metrics for recommender systems.\n",
      "\n",
      "- **Domain-Specific Metrics:** Metrics commonly used in specific industries, such as:\n",
      "  - Churn rate for telecoms.\n",
      "  - Word error rate in speech recognition.\n",
      "  - Fairness metrics in healthcare.\n",
      "\n",
      "- **Weighted or Aggregated Metrics:** Metrics that account for the importance of certain features, such as adjusting model quality metrics based on class balance or business importance ([Custom Metrics in ML Monitoring](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/custom-metrics-ml-monitoring.md)).\n",
      "\n",
      "### 3. Implementing Custom Metrics\n",
      "To create a custom metric in Evidently, you can refer to a practical guide or example notebook. This typically involves:\n",
      "\n",
      "- **Setting Up the Environment:** Import the Evidently library and necessary dependencies.\n",
      "  \n",
      "- **Creating a Custom Metric:** You would define your metric function based on your specific requirements. For example, you could implement a dummy metric first to understand the structure, and then develop a more complex metric like mean by category.\n",
      "\n",
      "- **Applying the Metric to Data:** Use your custom metric on your dataset to evaluate performance ([Implementing Custom Metrics Example](https://github.com/evidentlyai/ml_observability_course/blob/main/module4/custom_metric_practice.ipynb)).\n",
      "\n",
      "### 4. Consult with Stakeholders\n",
      "It's recommended to engage with business stakeholders during the metric design phase to capture valuable insights and ensure the metrics are aligned with business goals.\n",
      "\n",
      "### Additional Resources \n",
      "For a detailed walkthrough, you can check the optional coding practice video linked in the course materials, which demonstrates creating a custom metric and applying it to toy data ([Custom Metrics Implementation Video](https://youtu.be/uEyoP-sPhyc?si=7hwr4LaJIeBZ-YLD)).\n",
      "\n",
      "These steps should give you a solid foundation for implementing custom metrics effectively in the Evidently library. For further details, consult the references mentioned above.\n",
      "\n",
      "What data quality issues can occur in a complex ML system?\n",
      "In a complex machine learning (ML) system, several data quality issues can arise, jeopardizing the overall performance and reliability of the models. Key issues include:\n",
      "\n",
      "1. **Wrong Source**: Data pipelines may inadvertently point to obsolete or incorrect versions of databases or tables, leading to reliance on outdated information.\n",
      "\n",
      "2. **Lost Access**: Changes in permissions might result in loss of access to crucial data sources, disrupting the data flow necessary for model training and evaluation.\n",
      "\n",
      "3. **Bad SQL Queries**: Incorrectly constructed SQL queries or those affected by user-specific conditions (e.g., different time zones) can lead to failures in retrieving the right data.\n",
      "\n",
      "4. **Infrastructure Updates**: Updates to the underlying infrastructure or dependencies can inadvertently alter the way computations are carried out, affecting the consistency of data processing.\n",
      "\n",
      "5. **Broken Feature Code**: Issues may arise in feature computation, particularly for edge cases such as applying a 100% discount, leading to erroneous feature values.\n",
      "\n",
      "6. **Data Schema Changes**: If there are changes in the data schema without corresponding updates in the model or processing code, it can cause downstream issues in model performance.\n",
      "\n",
      "7. **Data Loss at the Source**: In-app logging might fail, or sensor values could freeze, resulting in missing data that is critical for operations.\n",
      "\n",
      "8. **Inter-model Dependencies**: In systems with multiple interacting models, failure of upstream models can result in incorrect input for downstream models, amplifying errors.\n",
      "\n",
      "These issues underscore the importance of rigorous data monitoring and quality assessments to ensure model outputs remain reliable and accurate. Regularly employing data profiling and monitoring techniques can help identify and rectify these potential issues before they impact model performance significantly.\n",
      "\n",
      "For more detailed information, refer to the source material on data quality in ML systems: [Data Quality in Machine Learning](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-2-ml-monitoring-metrics/data-quality-in-ml.md).\n",
      "\n",
      "How does monitoring data integrity metrics help in improving model performance?\n",
      "Monitoring data integrity metrics plays a crucial role in improving model performance for several reasons:\n",
      "\n",
      "1. **Detecting Data Quality Issues**: Monitoring metrics related to data integrity enables you to identify issues such as missing values, data type mismatches, and range violations in input features. These issues can directly affect the model's ability to make valuable predictions. By ensuring high data quality, you increase the model's chances of performing well [source](https://github.com/evidentlyai/ml_observability_course/blob/main/ml-observability-course-main/docs/book/ml-observability-course/module-1-introduction/ml-monitoring-metrics.md).\n",
      "\n",
      "2. **Early Detection of Problems**: Data integrity monitoring allows you to adopt early monitoring strategies that focus on input data and model output. For instance, metrics can be tracked over time to detect when data quality deteriorates or when theres significant drift in the input features. These early warning signs can help prevent the model from operating under poor conditions and thus maintain its performance [source](https://github.com/evidentlyai/ml_observability_course/blob/main/ml_observability_course-main/docs/book/ml-observability-course/module-2-ml-monitoring-metrics/evaluate-ml-model-quality.md).\n",
      "\n",
      "3. **Identifying Data Drift**: Monitoring includes tracking data drift, which refers to changes in the distribution of input features over time. If the input data changes significantly from what the model was trained on, it could lead to degraded performance. By regularly checking for drift, you can adjust the model or retrain it before performance issues become critical [source](https://github.com/evidentlyai/ml_observability_course/blob/main/ml-observability-course-main/docs/book/ml-observability-course/module-2-ml-monitoring-metrics/evaluate-ml-model-quality.md).\n",
      "\n",
      "4. **Proxy Metrics for Performance**: In situations where ground truth is not readily available, monitoring data integrity can provide proxy metrics. These metrics, including prediction drift and input data drift, can serve as indicators of when model performance might start to decline, enabling preemptive actions to mitigate potential degradation [source](https://github.com/evidentlyai/ml_observability_course/blob/main/ml_observability-course-main/docs/book/ml-observability-course/module-1-introduction/ml-monitoring-metrics.md).\n",
      "\n",
      "Overall, by keeping tabs on data integrity, you create a robust framework for ensuring that the underlying data remains consistent and reliable, ultimately improving the model's performance in real-world applications.\n",
      "\n",
      "What method can you use to capture and log per-column descriptive statistics in a monitoring setup?\n",
      "To capture and log per-column descriptive statistics in a monitoring setup, the following approach can be utilized:\n",
      "\n",
      "1. **Input/Output and Performance Profiling**: Include descriptive statistics for each feature in your model's inputs and outputs. This includes metrics like mean, median, variance, and quantiles for the data you are processing. These statistics provide valuable insights into the characteristics of the data being modeled and can help in detecting data drift or anomalies.\n",
      "\n",
      "2. **Monitoring Architecture**: Set up a logging architecture that allows you to record these detailed metrics. This may involve a prediction store where prediction-related logs (including descriptive statistics) are captured for analysis. Depending on your specific setup, you can either have an **ML monitoring service** that pulls these logs or directly push logs from your ML service to compute the necessary monitoring metrics.\n",
      "\n",
      "3. **Data Collection for Analysis**: Ensure that your logging includes comprehensive information, such as:\n",
      "   - Model input data,\n",
      "   - Model output,\n",
      "   - Ground truth data (if available),\n",
      "   - Per-column statistics to help summarize performance and data qualities over time.\n",
      "\n",
      "These logs can assist you not only in tracking model performance but also in understanding shifts in data distribution (data drift) and guiding potential model retraining efforts.\n",
      "\n",
      "For further details, you can refer to the material on logging architecture in the course: [Logging for ML monitoring](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/logging-ml-monitoring.md) and prioritizing metrics: [How to prioritize ML monitoring metrics](https://github.com/evidentlyai/ml_observability_course/blob/main/docs/book/ml-observability-course/module-4-designing-effective-ml-monitoring/how-to-prioritize-monitoring-metrics.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75310b08-7e51-44f1-803f-6a76420f75bf",
   "metadata": {},
   "source": [
    "**Collect AI-Generated Logs for Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314346c-70cf-4973-8edc-b782d85607ad",
   "metadata": {},
   "outputs": [],
   "source": "eval_set = []\n\nfor log_file in LOG_DIR.glob('*.json'):\n    if 'credit_risk_agent_v2' not in log_file.name:\n        continue\n\n    log_record = load_log_file(log_file)\n    if log_record['source'] != 'ai-generated':\n        continue\n\n    eval_set.append(log_record)"
  },
  {
   "cell_type": "markdown",
   "id": "78b2b112-25ba-4ce9-8223-e91afb71419b",
   "metadata": {},
   "source": [
    "**Run Evaluation on All Logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d964acfd-b638-49cc-a652-f4bf18c16556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b6ae6533714554901298a5a32c4a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860ad9b-1662-4cef-949b-44a6b3ca8372",
   "metadata": {},
   "source": [
    "**Convert Results to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "74c447d3-a1f0-4ec5-8f28-39d487345b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9dd98-46e0-4b68-883a-74090c6c4bd0",
   "metadata": {},
   "source": [
    "**View Results and Pass Rates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34781577-81de-46e0-a06f-70e89d1b332f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml_monitoring_agent_v2_20260212_230933_289659....</td>\n",
       "      <td>What is the difference between direct and prox...</td>\n",
       "      <td>In the context of monitoring model performance...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ml_monitoring_agent_v2_20260212_230959_a5651f....</td>\n",
       "      <td>What are some strategies for monitoring embedd...</td>\n",
       "      <td>When monitoring embedding drift in high-dimens...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ml_monitoring_agent_v2_20260212_230912_dbb955....</td>\n",
       "      <td>What are the key metrics to prioritize for eff...</td>\n",
       "      <td>To effectively monitor machine learning (ML) m...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ml_monitoring_agent_v2_20260212_231026_26470a....</td>\n",
       "      <td>How does monitoring data integrity metrics hel...</td>\n",
       "      <td>Monitoring data integrity metrics plays a cruc...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ml_monitoring_agent_v2_20260212_230940_ed0f31....</td>\n",
       "      <td>In what situations would minimalistic monitori...</td>\n",
       "      <td>Minimalistic monitoring for ML models is appro...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0  ml_monitoring_agent_v2_20260212_230933_289659....   \n",
       "1  ml_monitoring_agent_v2_20260212_230959_a5651f....   \n",
       "2  ml_monitoring_agent_v2_20260212_230912_dbb955....   \n",
       "3  ml_monitoring_agent_v2_20260212_231026_26470a....   \n",
       "4  ml_monitoring_agent_v2_20260212_230940_ed0f31....   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the difference between direct and prox...   \n",
       "1  What are some strategies for monitoring embedd...   \n",
       "2  What are the key metrics to prioritize for eff...   \n",
       "3  How does monitoring data integrity metrics hel...   \n",
       "4  In what situations would minimalistic monitori...   \n",
       "\n",
       "                                              answer  instructions_follow  \\\n",
       "0  In the context of monitoring model performance...                 True   \n",
       "1  When monitoring embedding drift in high-dimens...                 True   \n",
       "2  To effectively monitor machine learning (ML) m...                 True   \n",
       "3  Monitoring data integrity metrics plays a cruc...                 True   \n",
       "4  Minimalistic monitoring for ML models is appro...                 True   \n",
       "\n",
       "   instructions_avoid  answer_relevant  answer_clear  answer_citations  \\\n",
       "0                True             True          True              True   \n",
       "1                True             True          True              True   \n",
       "2                True             True          True              True   \n",
       "3                True             True          True              True   \n",
       "4                True             True          True              True   \n",
       "\n",
       "   completeness  tool_call_search  \n",
       "0          True              True  \n",
       "1          True              True  \n",
       "2          True              True  \n",
       "3          True              True  \n",
       "4          True              True  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb87e8e-d644-4cff-9502-002fb069faf5",
   "metadata": {},
   "source": [
    "**Overall Pass Rates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09ef68db-b7eb-4048-a40f-fa821b494e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    1.0\n",
       "instructions_avoid     1.0\n",
       "answer_relevant        1.0\n",
       "answer_clear           1.0\n",
       "answer_citations       1.0\n",
       "completeness           1.0\n",
       "tool_call_search       1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069afb6-3f19-40b0-a0e1-230067df7316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}